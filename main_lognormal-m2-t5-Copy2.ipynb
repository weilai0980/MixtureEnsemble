{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "from random import shuffle\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# local packages \n",
    "from utils_libs import *\n",
    "from utils_training import *\n",
    "from utils_inference import *\n",
    "from mixture_models import *\n",
    "\n",
    "# ------ GPU set-up in multi-GPU environment\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# ----- data and log paths\n",
    "arg_py = \"market2_tar1_len10\"\n",
    "path_data = \"../datasets/bitcoin/market2_tar1_len10/\"\n",
    "path_model = \"../results/m2_t1_2/\"\n",
    "path_py = \"../results/m2_t1_2/py_\" + arg_py\n",
    "path_log_error = \"../results/log_error_mix_\" + arg_py + \"_2\" + \".txt\"\n",
    "\n",
    "# ----- set-up\n",
    "\n",
    "# -- model\n",
    "\n",
    "para_distr_type = \"log_normal_linearOpt_linearComb\"\n",
    "para_distr_para = []\n",
    "para_var_type = \"exp\" # square, exp\n",
    "para_share_type_gate = \"no_share\"\n",
    "# no_share, share, mix\n",
    "para_model_type = 'linear'\n",
    "\n",
    "# -- data\n",
    "\n",
    "if para_model_type == 'rnn':\n",
    "    para_x_src_padding = False\n",
    "    para_add_common_factor = False\n",
    "    para_common_factor_type = \"pool\" if para_add_common_factor == True else \"\"\n",
    "    \n",
    "elif para_model_type == 'linear':\n",
    "    para_x_src_padding = True\n",
    "    para_add_common_factor = False\n",
    "    para_common_factor_type = \"factor\" if para_add_common_factor == True else \"\"\n",
    "\n",
    "para_bool_target_seperate = False # [Note] if yes, the last source corresponds to the auto-regressive target variable\n",
    "para_x_shape_acronym = [\"src\", \"N\", \"T\", \"D\"]\n",
    "\n",
    "# -- training\n",
    "\n",
    "# [Note] if best epoch is close to \"para_n_epoch\", possible to increase \"para_n_epoch\".\n",
    "# [Note] if best epoch is around the middle place of the training trajectory, ensemble expects to take effect. \n",
    "para_n_epoch = 90\n",
    "para_burn_in_epoch = 85\n",
    "para_vali_snapshot_num = max(1, int(0.05*para_n_epoch))\n",
    "para_test_snapshot_num = para_n_epoch - para_burn_in_epoch\n",
    "para_test_snapshot_sample_interval = 2\n",
    "\n",
    "para_hpara_search = \"random\" # random, grid \n",
    "para_hpara_train_trial_num = 30\n",
    "para_hpara_retrain_num = 10\n",
    "para_hpara_ensemble_num = 3\n",
    "\n",
    "# optimization\n",
    "para_loss_type = \"heter_lk_inv\"\n",
    "# \"heter_lk_inv\"\n",
    "para_optimizer = \"adam\"\n",
    "# RMSprop, adam, sgd, adamW \n",
    "# sg_mcmc_RMSprop, sg_mcmc_adam\n",
    "# [Note] for sg_mcmc family, \"para_n_epoch\" could be set to higher values\n",
    "\n",
    "# [Note] training heuristic: re-set the following for training on new data\n",
    "# [Note] if lr_decay is on, \"lr\" and \"para_n_epoch\" can be set to higher values\n",
    "para_optimizer_lr_decay = True \n",
    "para_optimizer_lr_decay_epoch = 10 # after the warm-up\n",
    "# [Note] when sg_mcmc is on, turn off the learning rate warm-up\n",
    "para_optimizer_lr_warmup_epoch = max(1, int(0.1*para_n_epoch))\n",
    "\n",
    "para_early_stop_bool = False\n",
    "para_early_stop_window = 0\n",
    "\n",
    "para_validation_metric = 'rmse'\n",
    "para_metric_map = {'rmse':0, 'mae':1, 'mape':2, 'nnllk':3}\n",
    "\n",
    "# regularization\n",
    "para_regu_mean = True\n",
    "para_regu_var = True\n",
    "para_regu_gate = False\n",
    "para_regu_mean_positive = False\n",
    "\n",
    "para_bool_bias_in_mean = True\n",
    "para_bool_bias_in_var = True\n",
    "para_bool_bias_in_gate = True\n",
    "\n",
    "# -- hpara: hyper parameter\n",
    "\n",
    "para_hpara_range = {}\n",
    "para_hpara_range['random'] = {}\n",
    "para_hpara_range['random']['linear'] = {}\n",
    "para_hpara_range['random']['rnn'] = {}\n",
    "\n",
    "# - linear\n",
    "if para_add_common_factor == True:\n",
    "    para_hpara_range['random']['linear']['factor_size'] = [10, 10]\n",
    "para_hpara_range['random']['linear']['lr'] = [1e-4, 1e-3]\n",
    "para_hpara_range['random']['linear']['batch_size'] = [10, 300]\n",
    "# source-wise\n",
    "para_hpara_range['random']['linear']['l2_mean'] = [1e-1, 5e-0]\n",
    "para_hpara_range['random']['linear']['l2_var'] =  [1e-1, 5e-0]\n",
    "\n",
    "# # - rnn\n",
    "# # source-wise\n",
    "# para_hpara_range['random']['rnn']['rnn_size'] =  [16, 16]\n",
    "# para_hpara_range['random']['rnn']['dense_num'] = [0, 3] # inproper value leads to non-convergence in training\n",
    "\n",
    "# para_hpara_range['random']['rnn']['lr'] = [0.001, 0.001]\n",
    "# para_hpara_range['random']['rnn']['batch_size'] = [100, 140]\n",
    "\n",
    "# # source-wise\n",
    "# para_hpara_range['random']['rnn']['l2_mean'] = [1e-7, 1e-3]\n",
    "# para_hpara_range['random']['rnn']['l2_var'] = [1e-7, 1e-3]\n",
    "# if para_regu_gate == True:\n",
    "#     para_hpara_range['random']['linear']['l2_gate'] = [1e-7, 1e-3]\n",
    "    \n",
    "# para_hpara_range['random']['rnn']['dropout_keep_prob'] = [0.7, 1.0]\n",
    "# para_hpara_range['random']['rnn']['max_norm_cons'] = [0.0, 0.0]\n",
    "\n",
    "# -- log\n",
    "def log_train(path):\n",
    "    with open(path, \"a\") as text_file:\n",
    "        text_file.write(\"\\n\\n ------ Bayesian mixture : \\n\")\n",
    "        \n",
    "        text_file.write(\"data source padding : %s \\n\"%(para_x_src_padding))\n",
    "        text_file.write(\"data path : %s \\n\"%(path_data))\n",
    "        text_file.write(\"data source timesteps : %s \\n\"%(para_steps_x))\n",
    "        text_file.write(\"data source feature dimensionality : %s \\n\"%(para_dim_x))\n",
    "        text_file.write(\"data source number : %d \\n\"%( len(src_ts_x) ))\n",
    "        text_file.write(\"data common factor : %s \\n\"%(para_add_common_factor))\n",
    "        text_file.write(\"data common factor type : %s \\n\"%(para_common_factor_type))\n",
    "        text_file.write(\"prediction path : %s \\n\"%(path_py))\n",
    "        text_file.write(\"\\n\")\n",
    "        \n",
    "        text_file.write(\"model type : %s \\n\"%(para_model_type))\n",
    "        text_file.write(\"target distribution type : %s \\n\"%(para_distr_type))\n",
    "        text_file.write(\"target distribution para. : %s \\n\"%(str(para_distr_para)))\n",
    "        text_file.write(\"target variable as a seperated data source : %s \\n\"%(para_bool_target_seperate))\n",
    "        text_file.write(\"variance calculation type : %s \\n\"%(para_var_type))\n",
    "        text_file.write(\"para. sharing in gate logit : %s \\n\"%(para_share_type_gate))\n",
    "        text_file.write(\"\\n\")\n",
    "        \n",
    "        text_file.write(\"regularization on mean : %s \\n\"%(para_regu_mean))\n",
    "        text_file.write(\"regularization on variance : %s \\n\"%(para_regu_var))\n",
    "        text_file.write(\"regularization on mixture gates : %s \\n\"%(para_regu_gate))\n",
    "        text_file.write(\"regularization on positive means : %s \\n\"%(para_regu_mean_positive))\n",
    "        text_file.write(\"\\n\")\n",
    "        \n",
    "        text_file.write(\"adding bias terms in mean : %s \\n\"%(para_bool_bias_in_mean))\n",
    "        text_file.write(\"adding bias terms in variance : %s \\n\"%(para_bool_bias_in_var))\n",
    "        text_file.write(\"adding bias terms in gates : %s \\n\"%(para_bool_bias_in_gate))\n",
    "        text_file.write(\"\\n\")\n",
    "        \n",
    "        text_file.write(\"optimizer : %s \\n\"%(para_optimizer))\n",
    "        text_file.write(\"loss type : %s \\n\"%(para_loss_type))\n",
    "        text_file.write(\"learning rate decay : %s \\n\"%(str(para_optimizer_lr_decay)))\n",
    "        text_file.write(\"learning rate decay epoch : %s \\n\"%(str(para_optimizer_lr_decay_epoch)))\n",
    "        text_file.write(\"learning rate warm-up epoch : %s \\n\"%(str(para_optimizer_lr_warmup_epoch)))\n",
    "        text_file.write(\"\\n\")\n",
    "        \n",
    "        text_file.write(\"hyper-para search : %s \\n\"%(para_hpara_search))\n",
    "        text_file.write(\"hyper-para range : %s \\n\"%(str(para_hpara_range[para_hpara_search][para_model_type])))\n",
    "        text_file.write(\"hyper-para training trial num : %s \\n\"%(str(para_hpara_train_trial_num)))\n",
    "        text_file.write(\"hyper-para retraining num.: %s \\n\"%(str(para_hpara_retrain_num)))\n",
    "        text_file.write(\"random seed ensemble num.: %s \\n\"%(str(para_hpara_ensemble_num)))\n",
    "        text_file.write(\"\\n\")\n",
    "        \n",
    "        text_file.write(\"epochs in total : %s \\n\"%(para_n_epoch))\n",
    "        text_file.write(\"burn_in_epoch : %s \\n\"%(para_burn_in_epoch))\n",
    "        text_file.write(\"num. snapshots in validating : %s \\n\"%(para_vali_snapshot_num))\n",
    "        text_file.write(\"num. snapshots in testing : %s \\n\"%(para_test_snapshot_num))\n",
    "        text_file.write(\"validation metric : %s \\n\"%(para_validation_metric))\n",
    "        text_file.write(\"early-stoping : %s \\n\"%(para_early_stop_bool))\n",
    "        text_file.write(\"early-stoping look-back window : %s \\n\"%(para_early_stop_window))\n",
    "        \n",
    "        text_file.write(\"\\n\\n\")\n",
    "\n",
    "# ----- training and evalution\n",
    "    \n",
    "def training_validating(xtr,\n",
    "                        ytr,\n",
    "                        xval,\n",
    "                        yval,\n",
    "                        dim_x,\n",
    "                        steps_x,\n",
    "                        hyper_para_dict,\n",
    "                        training_dict,\n",
    "                        retrain_top_steps, \n",
    "                        retrain_bayes_steps,\n",
    "                        retrain_bool,\n",
    "                        retrain_idx,\n",
    "                        random_seed):\n",
    "    '''\n",
    "    Argu.:\n",
    "      xtr: [num_src, N, T, D]\n",
    "         S: num_src\n",
    "         N: number of data samples\n",
    "         T: number of steps\n",
    "         D: dimension at each time step\n",
    "      ytr: [N 1]\n",
    "        \n",
    "      dim_x: integer, corresponding to D\n",
    "      steps_x: integer, corresponding to T\n",
    "      \n",
    "      hyper_para_dict: \n",
    "       \"lr\": float,\n",
    "       \"batch_size\": int\n",
    "       \"l2\": float,\n",
    "                           \n",
    "       \"lstm_size\": int,\n",
    "       \"dense_num\": int,\n",
    "       \"use_hidden_before_dense\": bool\n",
    "       \n",
    "      training_dict:\n",
    "       \"batch_per_epoch\": int\n",
    "       \"tr_idx\": list of integer\n",
    "    '''\n",
    "    # clear the graph in the current session \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.device('/device:GPU:0'):\n",
    "        \n",
    "        # clear the graph in the current session \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # fix the random seed to stabilize the network\n",
    "        os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "        random.seed(random_seed)  # `python` built-in pseudo-random generator\n",
    "        np.random.seed(random_seed)\n",
    "        tf.set_random_seed(random_seed)\n",
    "        \n",
    "        # session set-up\n",
    "        config = tf.ConfigProto()\n",
    "        config.allow_soft_placement = True\n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config = config)\n",
    "        \n",
    "        model = mixture_statistic(session = sess, \n",
    "                                  loss_type = para_loss_type,\n",
    "                                  num_src = len(xtr),\n",
    "                                  hyper_para_dict = hyper_para_dict, \n",
    "                                  model_type = para_model_type)\n",
    "        \n",
    "        # -- initialize the network\n",
    "        model.network_ini(hyper_para_dict,\n",
    "                          x_dim = dim_x,\n",
    "                          x_steps = steps_x, \n",
    "                          x_bool_common_factor = para_add_common_factor,\n",
    "                          model_type = para_model_type, \n",
    "                          model_distr_type = para_distr_type,\n",
    "                          model_distr_para = para_distr_para,\n",
    "                          model_var_type = para_var_type,\n",
    "                          model_para_share_type = para_share_type_gate,\n",
    "                          bool_regu_mean = para_regu_mean,\n",
    "                          bool_regu_var = para_regu_var,\n",
    "                          bool_regu_gate = para_regu_gate,\n",
    "                          bool_regu_positive_mean = para_regu_mean_positive,\n",
    "                          bool_bias_mean = para_bool_bias_in_mean,\n",
    "                          bool_bias_var = para_bool_bias_in_var,\n",
    "                          bool_bias_gate = para_bool_bias_in_gate,\n",
    "                          optimization_method = para_optimizer,\n",
    "                          optimization_lr_decay = para_optimizer_lr_decay,\n",
    "                          optimization_lr_decay_steps = para_optimizer_lr_decay_epoch*int(len(xtr[0])/int(hyper_para_dict[\"batch_size\"])),\n",
    "                          optimization_burn_in_step = para_burn_in_epoch,\n",
    "                          optimization_warmup_step = para_optimizer_lr_warmup_epoch*training_dict[\"batch_per_epoch\"] - 1)\n",
    "        \n",
    "        # !! the order of Saver\n",
    "        saver = tf.train.Saver(max_to_keep = None)\n",
    "        \n",
    "        model.train_ini()\n",
    "        model.inference_ini()\n",
    "        #tf.get_default_graph().finalize()\n",
    "        \n",
    "        # -- set up training batch parameters\n",
    "        batch_gen = data_loader(x = xtr,\n",
    "                                y = ytr,\n",
    "                                batch_size = int(hyper_para_dict[\"batch_size\"]), \n",
    "                                num_ins = training_dict[\"tr_num_ins\"],  \n",
    "                                num_src = len(xtr))\n",
    "        # -- begin training\n",
    "        \n",
    "        # training and validation error log\n",
    "        step_error = []\n",
    "        global_step = 0\n",
    "        \n",
    "        # training time counter\n",
    "        st_time = time.time()\n",
    "        \n",
    "        for epoch in range(para_n_epoch):\n",
    "            # shuffle traning instances each epoch\n",
    "            batch_gen.re_shuffle()\n",
    "            batch_x, batch_y, bool_last = batch_gen.one_batch()\n",
    "            \n",
    "            # loop over all batches\n",
    "            while batch_x != None:\n",
    "                    \n",
    "                # one-step training on a batch of training data\n",
    "                model.train_batch(batch_x, \n",
    "                                  batch_y,\n",
    "                                  global_step = epoch)\n",
    "                \n",
    "                # - batch-wise validation\n",
    "                # val_metric: [val_rmse, val_mae, val_mape, val_nnllk]\n",
    "                # nnllk: normalized negative log likelihood\n",
    "                val_metric, monitor_metric = model.validation(xval,\n",
    "                                                              yval,\n",
    "                                                              step = global_step,\n",
    "                                                              bool_end_of_epoch = bool_last)\n",
    "                if val_metric:\n",
    "                    # tr_metric [tr_rmse, tr_mae, tr_mape, tr_nnllk]\n",
    "                    tr_metric, _ = model.inference(xtr,\n",
    "                                                   ytr, \n",
    "                                                   bool_py_eval = False)\n",
    "                    #step_error.append([global_step, tr_metric, val_metric, epoch])\n",
    "                    step_error.append([epoch, tr_metric, val_metric, epoch])\n",
    "                    \n",
    "                # - next batch\n",
    "                batch_x, batch_y, bool_last = batch_gen.one_batch()\n",
    "                global_step += 1\n",
    "                    \n",
    "            # -- model saver \n",
    "            model_saver_flag = model.model_saver(path = path_model + para_model_type + '_' + str(retrain_idx) + '_' + str(epoch),\n",
    "                                                 epoch = epoch,\n",
    "                                                 step = global_step,\n",
    "                                                 top_snapshots = retrain_top_steps,\n",
    "                                                 bayes_snapshots = retrain_bayes_steps,\n",
    "                                                 early_stop_bool = para_early_stop_bool,\n",
    "                                                 early_stop_window = para_early_stop_window, \n",
    "                                                 tf_saver = saver)\n",
    "            # epoch-wise\n",
    "            print(\"\\n --- At epoch %d : \\n  %s \"%(epoch, str(step_error[-1])))\n",
    "            print(\"\\n   loss and regualization : \\n\", monitor_metric)\n",
    "            \n",
    "            # NAN value exception \n",
    "            if np.isnan(monitor_metric[0]) == True:\n",
    "                print(\"\\n --- NAN loss !! \\n\" )\n",
    "                break\n",
    "                \n",
    "            if retrain_bool == True and model_saver_flag != None:\n",
    "                print(\"\\n    [MODEL SAVED] \" + model_saver_flag + \" \\n \" + path_model + para_model_type + '_' + str(retrain_idx) + '_' + str(epoch))\n",
    "                \n",
    "        ed_time = time.time()\n",
    "        \n",
    "    # ? sorted training log ?\n",
    "    # step_error: [global_step, tr_metric, val_metric, epoch]\n",
    "    # sort step_error based on para_validation_metric\n",
    "    sort_step_error = sorted(step_error, key = lambda x:x[2][para_metric_map[para_validation_metric]])\n",
    "    \n",
    "    return sort_step_error,\\\n",
    "           1.0*(ed_time - st_time)/(epoch + 1e-5),\\\n",
    "\n",
    "# ----- main process  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # ------ data\n",
    "    \n",
    "    import pickle\n",
    "    tr_dta = pickle.load(open(path_data + 'train.p', \"rb\"), encoding = 'latin1')\n",
    "    val_dta = pickle.load(open(path_data + 'val.p', \"rb\"), encoding = 'latin1')\n",
    "    ts_dta = pickle.load(open(path_data + 'test.p', \"rb\"), encoding = 'latin1')\n",
    "    print(len(tr_dta), len(val_dta), len(ts_dta))\n",
    "    \n",
    "    # if para_bool_target_seperate = yes, the last source corresponds to the auto-regressive target variable\n",
    "    tr_x, tr_y = data_reshape(tr_dta, \n",
    "                              bool_target_seperate = para_bool_target_seperate)\n",
    "    val_x, val_y = data_reshape(val_dta,\n",
    "                                bool_target_seperate = para_bool_target_seperate)\n",
    "    ts_x, ts_y = data_reshape(ts_dta,\n",
    "                              bool_target_seperate = para_bool_target_seperate)\n",
    "    \n",
    "    # --- log transformation of y\n",
    "        \n",
    "    # output from the reshape\n",
    "    # y [N 1], x [S [N T D]]\n",
    "    print(\"training: \", len(tr_x[0]), len(tr_y))\n",
    "    print(\"validation: \", len(val_x[0]), len(val_y))\n",
    "    print(\"testing: \", len(ts_x[0]), len(ts_y))\n",
    "    \n",
    "    # --- source-wise data preparation \n",
    "\n",
    "    if para_x_src_padding == True:\n",
    "        # T and D different across data sources\n",
    "        # padding to same T and D\n",
    "        # y: [N 1], x: [S [N T D]]\n",
    "        src_tr_x = data_padding_x(tr_x,\n",
    "                                  num_src = len(tr_x))\n",
    "        src_val_x = data_padding_x(val_x,\n",
    "                                   num_src = len(tr_x))\n",
    "        src_ts_x = data_padding_x(ts_x,\n",
    "                                  num_src = len(tr_x))\n",
    "        print(\"Shapes after padding: \", np.shape(src_tr_x), np.shape(src_val_x), np.shape(src_ts_x))\n",
    "        \n",
    "    else:\n",
    "        src_tr_x = tr_x\n",
    "        src_val_x = val_x\n",
    "        src_ts_x = ts_x\n",
    "        \n",
    "    if para_add_common_factor == True:\n",
    "        # x: [S [N T D]]\n",
    "        # assume T is same across data sources\n",
    "        \n",
    "        # [N T sum(D)]\n",
    "        tr_x_concat = np.concatenate(tr_x, -1)\n",
    "        val_x_concat = np.concatenate(val_x, -1)\n",
    "        ts_x_concat = np.concatenate(ts_x, -1)\n",
    "        \n",
    "        if para_common_factor_type == \"pool\":\n",
    "            tr_x_factor = tr_x_concat\n",
    "            val_x_factor = val_x_concat\n",
    "            ts_x_factor = ts_x_concat\n",
    "            \n",
    "        elif para_common_factor_type == \"factor\":\n",
    "            tmp_dim = np.shape(tr_x_concat)[-1]\n",
    "            tmp_step = np.shape(tr_x_concat)[1]\n",
    "            \n",
    "            from sklearn.decomposition import FactorAnalysis\n",
    "            transformer = FactorAnalysis(n_components = 10, \n",
    "                                         random_state = 0)\n",
    "            # [N T d]\n",
    "            tr_x_factor = []\n",
    "            for tmp_x in tr_x_concat:\n",
    "                # tmp_x: [T sum(D)] -> [T d]\n",
    "                tr_x_factor.append(transformer.fit_transform(tmp_x))\n",
    "                \n",
    "            val_x_factor = []\n",
    "            for tmp_x in val_x_concat:\n",
    "                # tmp_x: [T sum(D)] -> [T d]\n",
    "                val_x_factor.append(transformer.fit_transform(tmp_x))\n",
    "            \n",
    "            ts_x_factor = []\n",
    "            for tmp_x in ts_x_concat:\n",
    "                # tmp_x: [T sum(D)] -> [T d]\n",
    "                ts_x_factor.append(transformer.fit_transform(tmp_x))\n",
    "        \n",
    "        # [S+1 [N T d]]\n",
    "        src_tr_x.append(np.asarray(tr_x_factor))\n",
    "        src_val_x.append(np.asarray(val_x_factor))\n",
    "        src_ts_x.append(np.asarray(ts_x_factor))\n",
    "    \n",
    "    # steps and dimensionality of each source\n",
    "    para_steps_x = []\n",
    "    para_dim_x = []\n",
    "    for tmp_src in range(len(src_tr_x)):\n",
    "        tmp_shape = np.shape(src_tr_x[tmp_src][0])\n",
    "        para_steps_x.append(tmp_shape[0])\n",
    "        para_dim_x.append(tmp_shape[1])\n",
    "        print(\"src \" + str(tmp_src) + \" shape: \", tmp_shape)\n",
    "    \n",
    "    shape_tr_x_dict = dict({\"N\": len(tr_x[0])})\n",
    "    \n",
    "    # ------ training and validation\n",
    "    \n",
    "    log_train(path_log_error)\n",
    "    \n",
    "    # -- hyper-para generator \n",
    "    if para_hpara_search == \"random\":        \n",
    "        hpara_generator = hyper_para_random_search(para_hpara_range[para_hpara_search][para_model_type], \n",
    "                                                   para_hpara_train_trial_num)\n",
    "    elif para_hpara_search == \"grid\":\n",
    "        hpara_generator = hyper_para_grid_search(para_hpara_range[para_hpara_search][para_model_type])\n",
    "            \n",
    "    # -- begin hyper-para search\n",
    "    hpara_log = []\n",
    "    \n",
    "    # sample one set-up of hyper-para\n",
    "    hpara_dict = hpara_generator.one_trial()\n",
    "                                                 \n",
    "    while hpara_dict != None:\n",
    "        \n",
    "        tr_dict = training_para_gen(shape_x_dict = shape_tr_x_dict, \n",
    "                                    hpara_dict = hpara_dict)\n",
    "        # hp_: hyper-parameter\n",
    "        # hp_step_error: [[step, train_metric, val_metric, epoch]]\n",
    "        hp_step_error, hp_epoch_time = training_validating(src_tr_x,\n",
    "                                                           tr_y,\n",
    "                                                           src_val_x,\n",
    "                                                           val_y,\n",
    "                                                           dim_x = para_dim_x,\n",
    "                                                           steps_x = para_steps_x,\n",
    "                                                           hyper_para_dict = hpara_dict,\n",
    "                                                           training_dict = tr_dict,\n",
    "                                                           retrain_bool = False,\n",
    "                                                           retrain_top_steps = [],\n",
    "                                                           retrain_bayes_steps = [],\n",
    "                                                           retrain_idx = 0,\n",
    "                                                           random_seed = 1)\n",
    "        \n",
    "        #[ dict{lr, batch, l2, ..., burn_in_steps}, [[step, tr_metric, val_metric, epoch]] ]\n",
    "        hpara_dict[\"burn_in_steps\"] = para_burn_in_epoch # tr_dict[\"batch_per_epoch\"] - 1\n",
    "        hpara_log.append([hpara_dict, hp_step_error])\n",
    "        \n",
    "        # -- prepare for the next trial\n",
    "        \n",
    "        # sample the next hyper-para\n",
    "        hpara_dict = hpara_generator.one_trial()\n",
    "        \n",
    "        # -- logging\n",
    "        log_train_val_performance(path_log_error,\n",
    "                                  hpara = hpara_log[-1][0],\n",
    "                                  hpara_error = hpara_log[-1][1][0],\n",
    "                                  train_time = hp_epoch_time)\n",
    "        # NAN loss exception\n",
    "        log_null_loss_exception(hp_step_error, \n",
    "                                path_log_error)\n",
    "        \n",
    "        print('\\n Validation performance under the hyper-parameters: \\n', hpara_log[-1][0], hpara_log[-1][1][0])\n",
    "        print('\\n Training time: \\n', hp_epoch_time, '\\n')\n",
    "        \n",
    "    # ------ re-train\n",
    "    #save all epoches in re-training, then select snapshots\n",
    "    \n",
    "    # best hyper-para\n",
    "    best_hpara, _, _, _, _ = hyper_para_selection(hpara_log, \n",
    "                                                  val_snapshot_num = para_vali_snapshot_num, \n",
    "                                                  test_snapshot_num = para_test_snapshot_num,\n",
    "                                                  metric_idx = para_metric_map[para_validation_metric])\n",
    "    retrain_hpara_steps = []\n",
    "    retrain_hpara_step_error = []\n",
    "    retrain_random_seeds = [1] + [randint(0, 1000) for _ in range(para_hpara_retrain_num-1)]\n",
    "    \n",
    "    for tmp_retrain_id in range(para_hpara_retrain_num):\n",
    "        \n",
    "        tr_dict = training_para_gen(shape_x_dict = shape_tr_x_dict,\n",
    "                                    hpara_dict = best_hpara)\n",
    "        \n",
    "        step_error, _ = training_validating(src_tr_x,\n",
    "                                            tr_y,\n",
    "                                            src_val_x,\n",
    "                                            val_y,\n",
    "                                            dim_x = para_dim_x,\n",
    "                                            steps_x = para_steps_x,\n",
    "                                            hyper_para_dict = best_hpara,\n",
    "                                            training_dict = tr_dict,\n",
    "                                            retrain_bool = True,\n",
    "                                            retrain_top_steps = list(range(para_n_epoch)), #top_steps,\n",
    "                                            retrain_bayes_steps = list(range(para_n_epoch)), #bayes_steps,\n",
    "                                            retrain_idx = tmp_retrain_id,\n",
    "                                            random_seed = retrain_random_seeds[tmp_retrain_id])\n",
    "        \n",
    "        top_steps, bayes_steps, top_steps_features, bayes_steps_features, val_error, step_error_pairs = snapshot_selection(train_log = step_error,\n",
    "                                                                                                                           snapshot_num = para_test_snapshot_num,\n",
    "                                                                                                                           total_step_num = para_n_epoch,\n",
    "                                                                                                                           metric_idx = para_metric_map[para_validation_metric],\n",
    "                                                                                                                           val_snapshot_num = para_vali_snapshot_num)\n",
    "        if len(top_steps) != 0:\n",
    "            retrain_hpara_steps.append([top_steps, bayes_steps, top_steps_features, bayes_steps_features, tmp_retrain_id, val_error])\n",
    "            retrain_hpara_step_error.append([step_error_pairs, tmp_retrain_id])\n",
    "        \n",
    "        log_val_hyper_para(path = path_log_error,\n",
    "                           hpara_tuple = [best_hpara, top_steps],\n",
    "                           error_tuple = step_error[0], \n",
    "                           log_string = \"-- \" + str(tmp_retrain_id))\n",
    "    \n",
    "        print('\\n----- Retrain hyper-parameters: ', best_hpara, top_steps, '\\n')\n",
    "        print('\\n----- Retrain validation performance: ', step_error[0], '\\n')\n",
    "    \n",
    "    sort_retrain_hpara_steps = sorted(retrain_hpara_steps, \n",
    "                                      key = lambda x:x[-1])\n",
    "    \n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [i[-2:] for i in sort_retrain_hpara_steps], \n",
    "                         ensemble_str = \"Retrain Ids and Vali. Errors: \")\n",
    "    \n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [i[-2:] for i in sort_retrain_hpara_steps[:para_hpara_ensemble_num]], \n",
    "                         ensemble_str = \"Retrain Ids for ensemble: \")\n",
    "    \n",
    "    # ------ testing\n",
    "    # error tuple: [rmse, mae, mape, nnllk]\n",
    "    # py_tuple\n",
    "    \n",
    "    # -- one snapshot from one retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [sort_retrain_hpara_steps[0][0][:1]],\n",
    "                                    retrain_ids = [ sort_retrain_hpara_steps[0][-2] ],\n",
    "                                    xts = src_ts_x, \n",
    "                                    yts = ts_y, \n",
    "                                    file_path = path_model, \n",
    "                                    bool_instance_eval = True,\n",
    "                                    loss_type = para_loss_type,\n",
    "                                    num_src = len(src_val_x),\n",
    "                                    snapshot_features = [],\n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [error_tuple], \n",
    "                         ensemble_str = \"One-shot-one-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_one_one\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- one snapshot from multi retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [tmp_steps[0][:1] for tmp_steps in sort_retrain_hpara_steps], \n",
    "                                    retrain_ids = [i[-2] for i in sort_retrain_hpara_steps[:para_hpara_ensemble_num]],\n",
    "                                    xts = src_ts_x,\n",
    "                                    yts = ts_y, \n",
    "                                    file_path = path_model,\n",
    "                                    bool_instance_eval = True,\n",
    "                                    loss_type = para_loss_type,\n",
    "                                    num_src = len(src_ts_x), \n",
    "                                    snapshot_features = [], \n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [error_tuple], \n",
    "                         ensemble_str = \"One-shot-multi-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_one_multi\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- top snapshots from one retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [sort_retrain_hpara_steps[0][0]], \n",
    "                                    retrain_ids = [ sort_retrain_hpara_steps[0][-2] ], \n",
    "                                    xts = src_ts_x, \n",
    "                                    yts = ts_y, \n",
    "                                    file_path = path_model,\n",
    "                                    bool_instance_eval = True, \n",
    "                                    loss_type = para_loss_type, \n",
    "                                    num_src = len(src_ts_x), \n",
    "                                    snapshot_features = [], \n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error,\n",
    "                         error_tuple = [error_tuple],\n",
    "                         ensemble_str = \"Top-shots-one-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_top_one\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- top snapshots multi retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [tmp_steps[0] for tmp_steps in sort_retrain_hpara_steps], \n",
    "                                    retrain_ids = [i[-2] for i in sort_retrain_hpara_steps[:para_hpara_ensemble_num]], \n",
    "                                    xts = src_ts_x,\n",
    "                                    yts = ts_y,\n",
    "                                    file_path = path_model,\n",
    "                                    bool_instance_eval = True,\n",
    "                                    loss_type = para_loss_type,\n",
    "                                    num_src = len(src_ts_x), \n",
    "                                    snapshot_features = [], \n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [error_tuple], \n",
    "                         ensemble_str = \"Top-shots-multi-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_top_multi\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- bayesian snapshots one retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [sort_retrain_hpara_steps[0][1]], \n",
    "                                    retrain_ids = [ sort_retrain_hpara_steps[0][-2] ], \n",
    "                                    xts = src_ts_x, \n",
    "                                    yts = ts_y,\n",
    "                                    file_path = path_model, \n",
    "                                    bool_instance_eval = True, \n",
    "                                    loss_type = para_loss_type, \n",
    "                                    num_src = len(src_ts_x), \n",
    "                                    snapshot_features = [], \n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [error_tuple], \n",
    "                         ensemble_str = \"Bayesian-one-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_bayes_one\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- bayesian snapshots multi retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [tmp_steps[1] for tmp_steps in sort_retrain_hpara_steps],\n",
    "                                    retrain_ids = [i[-2] for i in sort_retrain_hpara_steps[:para_hpara_ensemble_num]],\n",
    "                                    xts = src_ts_x,\n",
    "                                    yts = ts_y,\n",
    "                                    file_path = path_model,\n",
    "                                    bool_instance_eval = True,\n",
    "                                    loss_type = para_loss_type,\n",
    "                                    num_src = len(src_ts_x),\n",
    "                                    snapshot_features = [],\n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error,\n",
    "                         error_tuple = [error_tuple],\n",
    "                         ensemble_str = \"Bayesian-multi-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_bayes_multi\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- global top1 and topK steps\n",
    "    \n",
    "    retrain_ids, retrain_id_steps = global_top_steps_multi_retrain(retrain_step_error = retrain_hpara_step_error, \n",
    "                                                                   num_step = int(para_test_snapshot_num*para_hpara_ensemble_num))    \n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [retrain_ids, retrain_id_steps], \n",
    "                         ensemble_str = \"Global-top-steps: \")\n",
    "    \n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = retrain_id_steps, \n",
    "                                    retrain_ids = retrain_ids,\n",
    "                                    xts = src_ts_x,\n",
    "                                    yts = ts_y, \n",
    "                                    file_path = path_model,\n",
    "                                    bool_instance_eval = True,\n",
    "                                    loss_type = para_loss_type,\n",
    "                                    num_src = len(src_ts_x), \n",
    "                                    snapshot_features = [], \n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [error_tuple], \n",
    "                         ensemble_str = \"Global-top-steps-multi-retrain \")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_global\" + \".p\", \"wb\"))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
