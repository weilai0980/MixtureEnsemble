{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mplt \n",
    "from matplotlib import cm \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib \n",
    "from matplotlib.ticker import FuncFormatter \n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "\n",
    "from utils_libs import *\n",
    "# from utils_training import *\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    logsumexp trick\n",
    "    \n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "kde = KernelDensity(kernel = 'gaussian', bandwidth = 0.2).fit(X)\n",
    "kde_score = kde.score_samples(X)\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "print(kde_score)\n",
    "print(softmax_stable(kde_score, theta = 1.0, axis = None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- Contribution uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "from scipy.optimize import fmin_slsqp\n",
    "\n",
    "def func(p, r, xa, xb):\n",
    "    return truncnorm.nnlf(p, r)\n",
    "\n",
    "def constraint(p, r, xa, xb):\n",
    "    a, b, loc, scale = p\n",
    "    return np.array([a*scale + loc - xa, b*scale + loc - xb])\n",
    "\n",
    "real_l, real_r = 30, 250\n",
    "\n",
    "# --- ground truth data\n",
    "\n",
    "loc = 50\n",
    "scale = 75\n",
    "\n",
    "# normalized boundary\n",
    "a = (real_l - loc)/scale\n",
    "b = (real_r - loc)/scale\n",
    "\n",
    "# Generate some data to work with.\n",
    "r = truncnorm.rvs(a, \n",
    "                  b, \n",
    "                  loc = loc, \n",
    "                  scale = scale, \n",
    "                  size = 10000)\n",
    "# --- fit\n",
    "\n",
    "loc_guess = 30\n",
    "scale_guess = 90\n",
    "\n",
    "l_guess = (real_l - loc_guess)/scale_guess\n",
    "r_guess = (real_r - loc_guess)/scale_guess\n",
    "\n",
    "par = fmin_slsqp(func, \n",
    "                 [l_guess, r_guess, loc_guess, scale_guess], \n",
    "                 f_eqcons = constraint, \n",
    "                 args = (r, real_l, real_r),\n",
    "                 iprint = False, \n",
    "                 iter = 1000)\n",
    "print(p0)\n",
    "print(par)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xmin = 0\n",
    "xmax = 300\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x, truncnorm.pdf(x, a, b, loc=loc, scale=scale),\n",
    "        'r-', lw=3, alpha=0.4, label='truncnorm pdf')\n",
    "ax.plot(x, truncnorm.pdf(x, *par),\n",
    "        'k--', lw=1, alpha=1.0, label='truncnorm fit')\n",
    "ax.hist(r, bins=15, density=True, histtype='stepfilled', alpha=0.3)\n",
    "ax.legend(shadow=True)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# scale - exp μ, where μ is the mean of the log of the variate. (When fitting, typically you'd use the sample mean of the log of the data.)\n",
    "# shape - the standard deviation of the log of the variate.\n",
    "\n",
    "stats.lognorm(0.5, scale = np.exp(2), ).ppf(0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reshape(data,\n",
    "                 bool_target_seperate):\n",
    "    # S: source\n",
    "    # N: instance\n",
    "    # T: time steps\n",
    "    # D: dimensionality at each step\n",
    "    # data: [yi, ti, [xi_src1, xi_src2, ...]]\n",
    "    # by default, the first element in the xi_src1 is the auto-regressive target\n",
    "    \n",
    "    src_num = len(data[0][2])\n",
    "    tmpx = []\n",
    "    \n",
    "    if bool_target_seperate == True:\n",
    "        tmpx.append(np.asarray([tmp[2][0][:, 1:] for tmp in data]))\n",
    "        print(np.shape(tmpx[-1]))\n",
    "    \n",
    "        for src_idx in range(1, src_num):\n",
    "            tmpx.append(np.asarray([tmp[2][src_idx] for tmp in data]))\n",
    "            print(np.shape(tmpx[-1]))\n",
    "            \n",
    "        tmpx.append(np.asarray([tmp[2][0][:, 0:1] for tmp in data]))\n",
    "        print(np.shape(tmpx[-1]))\n",
    "    else:\n",
    "        for src_idx in range(src_num):\n",
    "            tmpx.append(np.asarray([tmp[2][src_idx] for tmp in data]))\n",
    "            print(\"src \" + str(src_idx) + \" : \", np.shape(tmpx[-1]))\n",
    "    \n",
    "    tmpy = np.asarray([tmp[0] for tmp in data])\n",
    "    \n",
    "    # output shape: x [S N T D],  y [N 1]\n",
    "    return tmpx, np.expand_dims(tmpy, -1)\n",
    "\n",
    "def mape(y, \n",
    "         yhat):\n",
    "    tmp_list = []\n",
    "    for idx, val in enumerate(y):\n",
    "        if abs(val) > 1e-5:\n",
    "            tmp_list.append(abs(1.0*(yhat[idx]-val)/val))\n",
    "    return np.mean(tmp_list)\n",
    "\n",
    "def mae(y,\n",
    "        yhat):\n",
    "    return np.mean(np.abs(np.asarray(y) - np.asarray(yhat)))\n",
    "    \n",
    "def rmse(y,\n",
    "         yhat):\n",
    "    return np.sqrt(np.mean((np.asarray(y) - np.asarray(yhat))**2))\n",
    "\n",
    "def rmse_mae_quantiles(target,\n",
    "                       pred,\n",
    "                       q):\n",
    "    '''\n",
    "    Argu.:\n",
    "      q: number of quantiles\n",
    "    '''\n",
    "    x_sort = sorted(list(target))\n",
    "    \n",
    "    quant_val = [x_sort[i*int(len(x_sort)/q)] for i in range(1, q)]\n",
    "    quant_val.append(max(x_sort)+1)\n",
    "\n",
    "    sq_error_quant = [0 for _ in quant_val]\n",
    "    abs_error_quant = [0 for _ in quant_val]\n",
    "    abs_per_error_quant = [0 for _ in quant_val]\n",
    "    \n",
    "    num_quant = [0 for x in quant_val]\n",
    "    \n",
    "    num_mape_quant = [0 for x in quant_val]\n",
    "    \n",
    "    for i in range(1, len(x_sort)):\n",
    "        \n",
    "        tmp_vol = target[i]\n",
    "        tmp_resi = target[i] - pred[i]\n",
    "        \n",
    "        q_idx = list(map(lambda j: j>tmp_vol, quant_val)).index(True)   \n",
    "        \n",
    "        sq_error_quant[q_idx] += tmp_resi**2\n",
    "        abs_error_quant[q_idx] += abs(tmp_resi)\n",
    "        num_quant[q_idx]+=1\n",
    "        \n",
    "        if abs(tmp_vol)>1e-5:\n",
    "            \n",
    "            abs_per_error_quant[q_idx] += abs(1.0*tmp_resi/tmp_vol)\n",
    "            num_mape_quant[q_idx] += 1\n",
    "        \n",
    "    rmse_quantiles = [ np.asscalar(np.squeeze(sqrt(sq_error_quant[j]/num_quant[j]))) for j in range(0, len(quant_val))] \n",
    "    mae_quantiles = [ np.asscalar(np.squeeze(abs_error_quant[j]/num_quant[j])) for j in range(0, len(quant_val))]\n",
    "    mape_quantiles = [ np.asscalar(np.squeeze(abs_per_error_quant[j]/num_mape_quant[j])) for j in range(0, len(quant_val))] \n",
    "    \n",
    "    print(\"cross check RMSE: \", np.sqrt(np.mean((np.squeeze(target) - pred)**2)), mae(np.squeeze(target), pred), mape(np.squeeze(target), pred))\n",
    "    \n",
    "    print(\"\\n quantile rmse: \", rmse_quantiles)\n",
    "    print(\"\\n quantile mae: \",  mae_quantiles)\n",
    "    print(\"\\n quantile mape: \", mape_quantiles)\n",
    "    print(\"\\n quantiles: \", quant_val)\n",
    "    \n",
    "    return rmse_quantiles, mae_quantiles, mape_quantiles, quant_val\n",
    "\n",
    "def rel_rmse_mae_quantiles(target,\n",
    "                           pred,\n",
    "                           q=4):\n",
    "    #not symmetric!!!\n",
    "    #q = number of quantiles\n",
    "\n",
    "    x_sort = sorted(list(target))\n",
    "\n",
    "    quant_val = [x_sort[i*int(len(x_sort)/q)] for i in range(1,q)]\n",
    "    quant_val.append(max(x_sort)+1)\n",
    "\n",
    "    sq_error_quantiles = [0 for x in quant_val]\n",
    "    error_num = [0 for x in quant_val]\n",
    "    abs_error_quantiles = [0 for x in quant_val]\n",
    "\n",
    "    for i in range(1, len(x_sort)):\n",
    "        tmp_vol = target[i]\n",
    "        tmp_rel_resi = (tmp_vol - pred[i]) / tmp_vol\n",
    "        \n",
    "        q_idx = list(map(lambda j: j>tmp_vol, quant_val)).index(True)\n",
    "        \n",
    "        sq_error_quantiles[q_idx] += tmp_rel_resi**2\n",
    "        abs_error_quantiles[q_idx] += abs(tmp_rel_resi)\n",
    "        error_num[q_idx]+=1\n",
    "\n",
    "    rmse_quantiles = [math.sqrt(sq_error_quantiles[j]/error_num[j]) for j in range(0, len(quant_val))]\n",
    "    mae_quantiles = [abs_error_quantiles[j]/error_num[j] for j in range(0, len(quant_val))]\n",
    "    \n",
    "    print(\"\\n quantile rel-rmse: \", rmse_quantiles)\n",
    "    print(\"\\n quantile rel-mae: \",  mae_quantiles)\n",
    "    print(\"\\n quantiles: \", quant_val)\n",
    "    \n",
    "    return rmse_quantiles, mae_quantiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def plot(py,\n",
    "#          y,\n",
    "#          plot_l,\n",
    "#          plot_r,\n",
    "#          path_fig,\n",
    "#          src_dict):\n",
    "#     # 4:\"commom pattern\"\n",
    "#     # [bayes_mean, bayes_total_var, bayes_vola, bayes_unc, bayes_gate_src, bayes_gate_src_var, g_src_sample]\n",
    "    \n",
    "#     mean = py[0]\n",
    "#     var = py[1]\n",
    "#     var_data = np.sqrt(py[2])\n",
    "#     var_model = np.sqrt(py[3])\n",
    "#     gate = py[4]\n",
    "#     gate_std = np.sqrt(py[5])\n",
    "    \n",
    "#     # [A B S]\n",
    "#     gate_src_sample = np.asarray(py[6])\n",
    "#     mean_src_sample = np.asarray(py[7])\n",
    "#     var_src_sample  = np.asarray(py[8])\n",
    "    \n",
    "#     mean_low = np.maximum(0.0, mean-2.0*np.sqrt(var))\n",
    "#     mean_up  = mean + 2.0*np.sqrt(var)\n",
    "    \n",
    "#     # -- source-wise\n",
    "    \n",
    "#     mean_src = np.mean(mean_src_sample, 0)\n",
    "#     tmp_var_plus_sq_mean_src = var_src_sample + mean_src_sample**2\n",
    "#     var_src = np.mean(tmp_var_plus_sq_mean_src, 0) - mean_src**2\n",
    "    \n",
    "#     mean_low_src = np.maximum(0.0, mean_src-2.0*np.sqrt(var_src))\n",
    "#     mean_up_src  = mean_src + 2.0*np.sqrt(var_src)\n",
    "        \n",
    "#     num_src = np.shape(gate)[1]\n",
    "    \n",
    "#     # -- cross check\n",
    "    \n",
    "#     y = np.asarray(np.squeeze(y))\n",
    "\n",
    "#     print(\"cross check RMSE: \", np.sqrt(np.mean((y - mean)**2)), mae(y, mean), mape(y, mean))\n",
    "    \n",
    "#     in_inter_sum = 0.0\n",
    "#     in_cnt = 0\n",
    "#     all_inter_sum = 0.0\n",
    "#     all_cnt = len(y)\n",
    "    \n",
    "#     for idx, tmpy in enumerate(y.tolist()):\n",
    "        \n",
    "#         if mean_low[idx] <= tmpy and tmpy <= mean_up[idx]:\n",
    "#             in_cnt += 1\n",
    "#             in_inter_sum += (mean_up[idx] - mean_low[idx])\n",
    "            \n",
    "#         all_inter_sum += (mean_up[idx] - mean_low[idx])\n",
    "            \n",
    "#     print(\"Interval coverage \", 1.0*in_cnt/len(y))\n",
    "#     print(\"all Interval width normalized by total number: \", 1.0*all_inter_sum/all_cnt)\n",
    "#     print(\"all Interval width normalized by coverage number: \", 1.0*all_inter_sum/in_cnt)\n",
    "#     print(\"within Interval width normalized by coverage number: \", in_inter_sum/in_cnt)\n",
    "    \n",
    "#     # -- mean and uncertainty\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize=(15,3));\n",
    "    \n",
    "#     ax.plot(range(plot_l, plot_r), y[plot_l: plot_r], label = 'truth', marker='o', color = 'k', alpha = 0.4);\n",
    "#     ax.plot(range(plot_l, plot_r), mean[plot_l: plot_r], label = 'prediction', marker='o', color = 'b', alpha = 0.4);\n",
    "#     ax.fill_between(range(plot_l, plot_r), mean_low[plot_l: plot_r], mean_up[plot_l: plot_r], \n",
    "#                     color = '#539caf', alpha = 0.4, label = '95.54% CI')\n",
    "#     #     ax_twin = ax.twinx()\n",
    "#     #     ax_twin.plot(range(plot_l, plot_r), vol[plot_l: plot_r], label = 'volatility',  marker='o', alpha=.3, color = 'g');\n",
    "    \n",
    "#     #ax2.tick_params(axis='y', labelcolor=color)\n",
    "#     #ax.plot(range(plot_l, plot_r), vol[plot_l: plot_r], label = 'volatility',  marker='o', alpha=.3, color = 'g');\n",
    "    \n",
    "#     ax.set_ylim(0, 1000)\n",
    "#     ax.set_title(\"Predicted means and uncertainties\");\n",
    "#     ax.set_xlabel(\"Time\");\n",
    "#     ax.set_ylabel(\"Value\");\n",
    "#     ax.legend();\n",
    "    \n",
    "#     ax.set_rasterized(True)\n",
    "#     # fig.savefig(path_fig + '_pred.eps', format = 'eps', bbox_inches = 'tight', dpi = 300)\n",
    "#     # fig.savefig(path_fig + '_pred.pdf', format = 'pdf', bbox_inches = 'tight', dpi = 100)\n",
    "    \n",
    "#     #  -- contribution of different sources with confidence interval\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize=(15, 3));\n",
    "    \n",
    "#     for tmp_src in range(num_src):\n",
    "#         tmp_y = [i[tmp_src] for i in gate[plot_l: plot_r]]\n",
    "#         tmp_yerr = [2*i[tmp_src] for i in gate_std[plot_l: plot_r]]\n",
    "#         ax.errorbar(range(plot_l, plot_r), tmp_y, yerr = tmp_yerr, marker='o', label = src_dict[tmp_src], capsize = 3);\n",
    "    \n",
    "#     ax.legend();\n",
    "#     ax.set_xlabel(\"Time\");\n",
    "#     ax.set_ylabel(\"Value\");\n",
    "#     ax.set_title(\"Contribution score and uncertainty of each source\");\n",
    "    \n",
    "#     ax.set_rasterized(True)\n",
    "#     fig.savefig(path_fig + '_src.pdf', format = 'pdf', bbox_inches = 'tight', dpi = 100)\n",
    "    \n",
    "#     # -- prediction interval of different sources\n",
    "    \n",
    "#     colors = ['b', 'r', 'y', 'g']\n",
    "    \n",
    "#     tmp_src = 0\n",
    "#     fig, ax = plt.subplots(figsize = (15, 3));\n",
    "#     ax.plot(range(plot_l, plot_r), y[plot_l: plot_r], label = 'truth', marker='o', color = 'k', alpha = 0.4);\n",
    "#     tmp_mean = [i[tmp_src] for i in mean_src]\n",
    "#     tmp_up = [i[tmp_src] for i in mean_up_src]\n",
    "#     tmp_low = [i[tmp_src] for i in mean_low_src]\n",
    "#     ax.plot(range(plot_l, plot_r), tmp_mean[plot_l: plot_r], label = 'prediction', marker='o', color = 'b', alpha = 0.4); \n",
    "#     ax.fill_between(range(plot_l, plot_r), tmp_low[plot_l: plot_r], tmp_up[plot_l: plot_r], color = '#539caf', alpha = 0.4, label = '')\n",
    "#     ax.legend();\n",
    "#     ax.set_xlabel(\"Time\");\n",
    "#     ax.set_ylabel(\"Value\");\n",
    "#     ax.set_title(\"\");\n",
    "#     ax.set_rasterized(True)\n",
    "    \n",
    "#     tmp_src = 1\n",
    "#     fig, ax = plt.subplots(figsize = (15, 3));\n",
    "#     ax.plot(range(plot_l, plot_r), y[plot_l: plot_r], label = 'truth', marker='o', color = 'k', alpha = 0.4);    \n",
    "#     tmp_mean = [i[tmp_src] for i in mean_src]\n",
    "#     tmp_up = [i[tmp_src] for i in mean_up_src]\n",
    "#     tmp_low = [i[tmp_src] for i in mean_low_src]\n",
    "#     ax.plot(range(plot_l, plot_r), tmp_mean[plot_l: plot_r], label = 'prediction', marker='o', color = 'b', alpha = 0.4); \n",
    "#     ax.fill_between(range(plot_l, plot_r), tmp_low[plot_l: plot_r], tmp_up[plot_l: plot_r], color = '#539caf', alpha = 0.4, label = '')\n",
    "#     ax.legend();\n",
    "#     ax.set_xlabel(\"Time\");\n",
    "#     ax.set_ylabel(\"Value\");\n",
    "#     ax.set_title(\"\");\n",
    "#     ax.set_rasterized(True)\n",
    "    \n",
    "#     tmp_src = 2\n",
    "#     fig, ax = plt.subplots(figsize = (15, 3));\n",
    "#     ax.plot(range(plot_l, plot_r), y[plot_l: plot_r], label = 'truth', marker='o', color = 'k', alpha = 0.4);\n",
    "#     tmp_mean = [i[tmp_src] for i in mean_src]\n",
    "#     tmp_up = [i[tmp_src] for i in mean_up_src]\n",
    "#     tmp_low = [i[tmp_src] for i in mean_low_src]\n",
    "#     ax.plot(range(plot_l, plot_r), tmp_mean[plot_l: plot_r], label = 'prediction', marker='o', color = 'b', alpha = 0.4); \n",
    "#     ax.fill_between(range(plot_l, plot_r), tmp_low[plot_l: plot_r], tmp_up[plot_l: plot_r], color = '#539caf', alpha = 0.4, label = '')\n",
    "#     ax.legend();\n",
    "#     ax.set_xlabel(\"Time\");\n",
    "#     ax.set_ylabel(\"Value\");\n",
    "#     ax.set_title(\"\");\n",
    "#     ax.set_rasterized(True)\n",
    "    \n",
    "#     tmp_src = 3\n",
    "#     fig, ax = plt.subplots(figsize = (15, 3));\n",
    "#     ax.plot(range(plot_l, plot_r), y[plot_l: plot_r], label = 'truth', marker='o', color = 'k', alpha = 0.4);\n",
    "#     tmp_mean = [i[tmp_src] for i in mean_src]\n",
    "#     tmp_up = [i[tmp_src] for i in mean_up_src]\n",
    "#     tmp_low = [i[tmp_src] for i in mean_low_src]\n",
    "#     ax.plot(range(plot_l, plot_r), tmp_mean[plot_l: plot_r], label = 'prediction', marker='o', color = 'b', alpha = 0.4); \n",
    "#     ax.fill_between(range(plot_l, plot_r), tmp_low[plot_l: plot_r], tmp_up[plot_l: plot_r], color = '#539caf', alpha = 0.4, label = '')\n",
    "#     ax.legend();\n",
    "#     ax.set_xlabel(\"Time\");\n",
    "#     ax.set_ylabel(\"Value\");\n",
    "#     ax.set_title(\"\");\n",
    "#     ax.set_rasterized(True)\n",
    "#     #     fig.savefig(path_fig + '_src.pdf', format = 'pdf', bbox_inches = 'tight', dpi = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def data_source_switch(gate,\n",
    "                  gate_std,\n",
    "                  gate_src_sample, \n",
    "                  mean_src_sample, \n",
    "                  var_src_sample):\n",
    "    \n",
    "    num_ins = len(gate_src_sample[0])\n",
    "    num_samp = len(gate_src_sample)\n",
    "    \n",
    "    for i in range(num_ins):\n",
    "        tmp = gate[i]\n",
    "        tmp0 = tmp[0]\n",
    "        tmp[0] = tmp[1]\n",
    "        tmp[1] = tmp0\n",
    "        tmp2 = tmp[2]\n",
    "        tmp[2] = tmp[3]\n",
    "        tmp[3] = tmp2\n",
    "        \n",
    "        tmp = gate_std[i]\n",
    "        tmp0 = tmp[0]\n",
    "        tmp[0] = tmp[1]\n",
    "        tmp[1] = tmp0\n",
    "        tmp2 = tmp[2]\n",
    "        tmp[2] = tmp[3]\n",
    "        tmp[3] = tmp2\n",
    "            \n",
    "    for j in range(num_samp):\n",
    "        for i in range(num_ins):\n",
    "            tmp = gate_src_sample[j][i]\n",
    "            tmp0 = tmp[0]\n",
    "            tmp[0] = tmp[1]\n",
    "            tmp[1] = tmp0\n",
    "            tmp2 = tmp[2]\n",
    "            tmp[2] = tmp[3]\n",
    "            tmp[3] = tmp2\n",
    "        \n",
    "            tmp = mean_src_sample[j][i]\n",
    "            tmp0 = tmp[0]\n",
    "            tmp[0] = tmp[1]\n",
    "            tmp[1] = tmp0\n",
    "            tmp2 = tmp[2]\n",
    "            tmp[2] = tmp[3]\n",
    "            tmp[3] = tmp2\n",
    "            \n",
    "            tmp = var_src_sample[j][i]\n",
    "            tmp0 = tmp[0]\n",
    "            tmp[0] = tmp[1]\n",
    "            tmp[1] = tmp0\n",
    "            tmp2 = tmp[2]\n",
    "            tmp[2] = tmp[3]\n",
    "            tmp[3] = tmp2\n",
    "\n",
    "def plot_one(py,\n",
    "         y,\n",
    "         plot_l,\n",
    "         plot_r,\n",
    "         path_fig,\n",
    "         src_dict,\n",
    "         yrange, \n",
    "         market_id):\n",
    "    # 4:\"commom pattern\"\n",
    "    # [bayes_mean, bayes_total_var, bayes_vola, bayes_unc, bayes_gate_src, bayes_gate_src_var, g_src_sample]\n",
    "    \n",
    "    mean = py[0]\n",
    "    var = py[1]\n",
    "    var_data = np.sqrt(py[2])\n",
    "    var_model = np.sqrt(py[3])\n",
    "    # [B S]\n",
    "    gate = py[4]\n",
    "    gate_std = np.sqrt(py[5])\n",
    "    # [A B S]\n",
    "    gate_src_sample = py[6]\n",
    "    mean_src_sample = py[7]\n",
    "    var_src_sample  = py[8]\n",
    "    \n",
    "    if market_id == 2:\n",
    "        data_source_switch(gate, \n",
    "                           gate_std, \n",
    "                           gate_src_sample, \n",
    "                           mean_src_sample, \n",
    "                           var_src_sample)\n",
    "        \n",
    "    mean_low = np.maximum(0.0, mean - 2.0*np.sqrt(var))\n",
    "    mean_up  = np.minimum(800, mean + 2.0*np.sqrt(var))\n",
    "    \n",
    "    # -- source-wise\n",
    "    \n",
    "    gate_src_sample = np.asarray(gate_src_sample)\n",
    "    mean_src_sample = np.asarray(mean_src_sample)\n",
    "    var_src_sample  = np.asarray(var_src_sample)\n",
    "    \n",
    "    mean_src = np.mean(mean_src_sample, 0)\n",
    "    tmp_var_plus_sq_mean_src = var_src_sample + mean_src_sample**2\n",
    "    var_src = np.mean(tmp_var_plus_sq_mean_src, 0) - mean_src**2\n",
    "    \n",
    "    mean_low_src = np.maximum(0.0, mean_src-2.0*np.sqrt(var_src))\n",
    "    mean_up_src  = mean_src + 2.0*np.sqrt(var_src)\n",
    "    \n",
    "    num_src = np.shape(gate)[1]\n",
    "    \n",
    "    # -- cross check\n",
    "    \n",
    "    y = np.asarray(np.squeeze(y))\n",
    "\n",
    "    print(\"cross check RMSE: \", np.sqrt(np.mean((y - mean)**2)), mae(y, mean), mape(y, mean))\n",
    "    \n",
    "    in_inter_sum = 0.0\n",
    "    in_cnt = 0\n",
    "    all_inter_sum = 0.0\n",
    "    all_cnt = len(y)\n",
    "    \n",
    "    for idx, tmpy in enumerate(y.tolist()):\n",
    "        \n",
    "        if mean_low[idx] <= tmpy and tmpy <= mean_up[idx]:\n",
    "            in_cnt += 1\n",
    "            in_inter_sum += (mean_up[idx] - mean_low[idx])\n",
    "            \n",
    "        all_inter_sum += (mean_up[idx] - mean_low[idx])\n",
    "            \n",
    "    print(\"Interval coverage \", 1.0*in_cnt/len(y))\n",
    "    print(\"all Interval width normalized by total number: \", 1.0*all_inter_sum/all_cnt)\n",
    "    print(\"all Interval width normalized by coverage number: \", 1.0*all_inter_sum/in_cnt)\n",
    "    print(\"within Interval width normalized by coverage number: \", in_inter_sum/in_cnt)\n",
    "    \n",
    "    # -- mean and uncertainty\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(25, 40), nrows = 6);\n",
    "    \n",
    "    ax[0].plot(range(plot_l, plot_r), y[plot_l: plot_r], label = 'truth', marker='o', color = 'k', alpha = 0.4,  markersize = 10);\n",
    "    ax[0].plot(range(plot_l, plot_r), mean[plot_l: plot_r], label = 'prediction', marker='o', color = 'b', alpha = 0.4,  markersize = 10);\n",
    "    ax[0].fill_between(range(plot_l, plot_r), mean_low[plot_l: plot_r], mean_up[plot_l: plot_r], color = '#539caf', alpha = 0.4, label = 'uncertainty')\n",
    "    #     ax_twin = ax.twinx()\n",
    "    #     ax_twin.plot(range(plot_l, plot_r), vol[plot_l: plot_r], label = 'volatility',  marker='o', alpha=.3, color = 'g');\n",
    "    #ax2.tick_params(axis='y', labelcolor=color)\n",
    "    #ax.plot(range(plot_l, plot_r), vol[plot_l: plot_r], label = 'volatility',  marker='o', alpha=.3, color = 'g');\n",
    "    \n",
    "    ax[0].set_ylim(yrange[0], yrange[1])\n",
    "    # ax[0].set_title(); Predicted means and uncertainties\n",
    "    ax[0].set_xlabel('(a)', fontsize = 30);\n",
    "    ax[0].set_ylabel(\"Overall Prediction\", fontsize = 25);\n",
    "    ax[0].legend(fontsize = 25);\n",
    "    ax[0].set_rasterized(True)\n",
    "    ax[0].xaxis.set_tick_params(labelsize = 25)\n",
    "    ax[0].yaxis.set_tick_params(labelsize = 25)\n",
    "    ax[0].set_xticks(      list(range(plot_l, plot_r, 10)) )\n",
    "    ax[0].set_xticklabels( list(range(0, plot_r-plot_l, 10)) )\n",
    "    \n",
    "    #  -- contribution score\n",
    "    \n",
    "    colors = ['b', 'r', 'y', 'darkviolet']\n",
    "    \n",
    "    for tmp_src in range(num_src):\n",
    "        tmp_y = [i[tmp_src] for i in gate[plot_l: plot_r]]\n",
    "        tmp_yerr = [2*i[tmp_src] for i in gate_std[plot_l: plot_r]]\n",
    "        ax[1].errorbar(range(plot_l, plot_r), tmp_y, yerr = tmp_yerr, marker='o', label = src_dict[tmp_src], capsize = 3, color = colors[tmp_src], alpha = 0.4,  markersize = 10);\n",
    "    \n",
    "    ax[1].legend(fontsize = 25);\n",
    "    ax[1].set_xlabel(\"(b)\", fontsize = 30);\n",
    "    ax[1].set_ylabel(\"Contribution Score\", fontsize = 25);\n",
    "    #ax[1].set_title(\"Contribution score of each source\"); Contribution score of each source\n",
    "    ax[1].set_rasterized(True)\n",
    "    ax[1].xaxis.set_tick_params(labelsize = 25)\n",
    "    ax[1].yaxis.set_tick_params(labelsize = 25)\n",
    "    ax[1].set_xticks(      list(range(plot_l, plot_r, 10)) )\n",
    "    ax[1].set_xticklabels( list(range(0, plot_r-plot_l, 10)) )\n",
    "    \n",
    "    # -- prediction interval of different sources\n",
    "    \n",
    "    tmp_src = 0\n",
    "    ax[2].plot(range(plot_l, plot_r), y[plot_l: plot_r], label = 'truth', marker='o', color = 'k', alpha = 0.4, markersize = 10);\n",
    "    tmp_mean = [i[tmp_src] for i in mean_src]\n",
    "    tmp_up = [i[tmp_src] for i in mean_up_src]\n",
    "    tmp_low = [i[tmp_src] for i in mean_low_src]\n",
    "    ax[2].plot(range(plot_l, plot_r), tmp_mean[plot_l: plot_r], label = 'prediction by ' + src_dict[tmp_src], marker='o', color = colors[tmp_src], alpha = 0.4, markersize = 10);\n",
    "    ax[2].fill_between(range(plot_l, plot_r), tmp_low[plot_l: plot_r], tmp_up[plot_l: plot_r], color = colors[tmp_src], alpha = 0.4); \n",
    "    ax[2].legend(fontsize = 25);\n",
    "    ax[2].set_xlabel(\"(c)\", fontsize = 30);\n",
    "    ax[2].set_ylabel(\"Prediction by One Data Source\", fontsize = 25);\n",
    "    #     ax[2].set_title(\"\");\n",
    "    ax[2].set_rasterized(True)\n",
    "    ax[2].set_ylim(yrange[0], yrange[1])\n",
    "    ax[2].xaxis.set_tick_params(labelsize = 25)\n",
    "    ax[2].yaxis.set_tick_params(labelsize = 25)\n",
    "    ax[2].set_xticks(      list(range(plot_l, plot_r, 10)) )\n",
    "    ax[2].set_xticklabels( list(range(0, plot_r-plot_l, 10)) )\n",
    "    \n",
    "    tmp_src = 1\n",
    "    ax[3].plot(range(plot_l, plot_r), y[plot_l: plot_r], label = 'truth', marker='o', color = 'k', alpha = 0.4, markersize = 10);   \n",
    "    tmp_mean = [i[tmp_src] for i in mean_src]\n",
    "    tmp_up = [i[tmp_src] for i in mean_up_src]\n",
    "    tmp_low = [i[tmp_src] for i in mean_low_src]\n",
    "    ax[3].plot(range(plot_l, plot_r), tmp_mean[plot_l: plot_r], label = 'prediction by ' + src_dict[tmp_src], marker='o', color = colors[tmp_src], alpha = 0.4, markersize = 10); \n",
    "    ax[3].fill_between(range(plot_l, plot_r), tmp_low[plot_l: plot_r], tmp_up[plot_l: plot_r], label = '', color = colors[tmp_src], alpha = 0.4);  \n",
    "    ax[3].legend(fontsize = 25);\n",
    "    ax[3].set_xlabel(\"(d)\", fontsize = 30);\n",
    "    ax[3].set_ylabel(\"Prediction by One Data Source\", fontsize = 25);\n",
    "    #     ax[3].set_title(\"\");\n",
    "    ax[3].set_rasterized(True)\n",
    "    ax[3].set_ylim(yrange[0], yrange[1])\n",
    "    ax[3].xaxis.set_tick_params(labelsize = 25)\n",
    "    ax[3].yaxis.set_tick_params(labelsize = 25)\n",
    "    ax[3].set_xticks(      list(range(plot_l, plot_r, 10)) )\n",
    "    ax[3].set_xticklabels( list(range(0, plot_r-plot_l, 10)) )\n",
    "    \n",
    "    tmp_src = 2\n",
    "    ax[4].plot(range(plot_l, plot_r), y[plot_l: plot_r], label = 'truth', marker='o', color = 'k', alpha = 0.4, markersize = 10);\n",
    "    tmp_mean = [i[tmp_src] for i in mean_src]\n",
    "    tmp_up = [i[tmp_src] for i in mean_up_src]\n",
    "    tmp_low = [i[tmp_src] for i in mean_low_src]\n",
    "    ax[4].plot(range(plot_l, plot_r), tmp_mean[plot_l: plot_r], label = 'prediction by ' + src_dict[tmp_src], marker='o', color = colors[tmp_src], alpha = 0.4, markersize = 10); \n",
    "    ax[4].fill_between(range(plot_l, plot_r), tmp_low[plot_l: plot_r], tmp_up[plot_l: plot_r], color = colors[tmp_src], alpha = 0.4); \n",
    "    ax[4].legend(fontsize = 25);\n",
    "    ax[4].set_xlabel(\"(e)\", fontsize = 30);\n",
    "    ax[4].set_ylabel(\"Prediction by One Data Source\", fontsize = 25);\n",
    "    #     ax[4].set_title(\"\");\n",
    "    ax[4].set_rasterized(True)\n",
    "    ax[4].set_ylim(yrange[0], yrange[1])\n",
    "    ax[4].xaxis.set_tick_params(labelsize = 25)\n",
    "    ax[4].yaxis.set_tick_params(labelsize = 25)\n",
    "    ax[4].set_xticks(      list(range(plot_l, plot_r, 10)) )\n",
    "    ax[4].set_xticklabels( list(range(0, plot_r-plot_l, 10)) )\n",
    "    \n",
    "    tmp_src = 3\n",
    "    ax[5].plot(range(plot_l, plot_r), y[plot_l: plot_r], label = 'truth', marker='o', color = 'k', alpha = 0.4, markersize = 10);\n",
    "    tmp_mean = [i[tmp_src] for i in mean_src]\n",
    "    tmp_up = [i[tmp_src] for i in mean_up_src]\n",
    "    tmp_low = [i[tmp_src] for i in mean_low_src]\n",
    "    ax[5].plot(range(plot_l, plot_r), tmp_mean[plot_l: plot_r], label = 'prediction by ' + src_dict[tmp_src], marker='o', color = colors[tmp_src], alpha = 0.4, markersize = 10);\n",
    "    ax[5].fill_between(range(plot_l, plot_r), tmp_low[plot_l: plot_r], tmp_up[plot_l: plot_r], color = colors[tmp_src], alpha = 0.4); \n",
    "    ax[5].legend(fontsize = 25);\n",
    "    ax[5].set_xlabel(\"(f)\", fontsize = 30);\n",
    "    ax[5].set_ylabel(\"Prediction by One Data Source\", fontsize = 25);\n",
    "    #     ax[5].set_title(\"\");\n",
    "    ax[5].set_rasterized(True)\n",
    "    ax[5].set_ylim(yrange[0], yrange[1])\n",
    "    ax[5].xaxis.set_tick_params(labelsize = 25)\n",
    "    ax[5].yaxis.set_tick_params(labelsize = 25)\n",
    "    ax[5].set_xticks(      list(range(plot_l, plot_r, 10)) )\n",
    "    ax[5].set_xticklabels( list(range(0, plot_r-plot_l, 10)) )\n",
    "    \n",
    "#     fig.savefig(path_fig + '.pdf', format = 'pdf', bbox_inches = 'tight', dpi = 300)\n",
    "#     fig.savefig(path_fig + '.eps', format = 'eps', bbox_inches = 'tight', dpi = 300)\n",
    "    fig.savefig(path_fig + '.svg', format = 'svg', bbox_inches = 'tight', dpi = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Market 1 target 1min\n",
    "\n",
    "# ground truth data\n",
    "dataset = 'market1_tar1_len10'\n",
    "path_data = \"../datasets/bitcoin/market1_tar1_len10/\"\n",
    "ts_dta = pickle.load(open(path_data + 'test.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "ts_x, ts_y = data_reshape(ts_dta,\n",
    "                          bool_target_seperate = True)\n",
    "# prediction\n",
    "path_res = \"../results/m1_t1_1/\"\n",
    "# py: [bayes_mean, bayes_total_var, bayes_vola, bayes_unc, bayes_gate_src, bayes_gate_src_var, g_src_sample]\n",
    "py = pickle.load(open(path_res + \"py_\" + dataset + \"_top_one\" + \".p\", \"rb\"), \n",
    "                 encoding='latin1')\n",
    "\n",
    "# src_dict = {0:\"Bitfinex trans.\", \n",
    "#             1:\"Bitstamp trans.\", \n",
    "#             2:\"Bitfinex order book\", \n",
    "#             3:\"Bitstamp order book\", \n",
    "#             4:\"commom pattern\"}\n",
    "# plot_one(py,\n",
    "#          y = ts_y,\n",
    "#          plot_l = 15450,\n",
    "#          plot_r = 15600,\n",
    "#          path_fig = \"../figure_volume/m1_t1\",\n",
    "#          src_dict = src_dict,\n",
    "#          yrange = [0, 400], \n",
    "#          market_id = 1)\n",
    "\n",
    "_ = rmse_mae_quantiles(target = np.squeeze(ts_y), \n",
    "                   pred = py[0], \n",
    "                   q = 4)\n",
    "\n",
    "_ = rel_rmse_mae_quantiles(target = np.squeeze(ts_y), \n",
    "                       pred = py[0],\n",
    "                       q=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Market 2 target 1min\n",
    "\n",
    "# ground truth data\n",
    "dataset = 'market2_tar1_len10'\n",
    "path_data = \"../datasets/bitcoin/market2_tar1_len10/\"\n",
    "ts_dta = pickle.load(open(path_data + 'test.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "ts_x, ts_y = data_reshape(ts_dta,\n",
    "                          bool_target_seperate = True)\n",
    "# prediction\n",
    "path_res = \"../results/m2_t1_1/\"\n",
    "# py: [bayes_mean, bayes_total_var, bayes_vola, bayes_unc, bayes_gate_src, bayes_gate_src_var, g_src_sample]\n",
    "py = pickle.load(open(path_res + \"py_\" + dataset + \"_top_one\" + \".p\", \"rb\"), \n",
    "                 encoding='latin1')\n",
    "\n",
    "# src_dict = {0:\"Bitfinex trans.\", \n",
    "#             1:\"Bitstamp trans.\", \n",
    "#             2:\"Bitfinex order book\", \n",
    "#             3:\"Bitstamp order book\", \n",
    "#             4:\"commom pattern\"}\n",
    "# plot_one(py,\n",
    "#          y = ts_y,\n",
    "#          plot_l = 29150,\n",
    "#          plot_r = 29300, \n",
    "#          path_fig = \"../figure_volume/m2_t1\", \n",
    "#          src_dict = src_dict, \n",
    "#          yrange = [0, 100], \n",
    "#          market_id = 2)\n",
    "\n",
    "rmse_mae_quantiles(target = ts_y, pred = py[0], q = 4)\n",
    "\n",
    "rel_rmse_mae_quantiles(target = np.squeeze(ts_y), \n",
    "                       pred = py[0],\n",
    "                       q=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Market 1 target 5min\n",
    "\n",
    "# ground truth data\n",
    "dataset = 'market1_tar5_len10'\n",
    "path_data = \"../datasets/bitcoin/market1_tar5_len10/\"\n",
    "ts_dta = pickle.load(open(path_data + 'test.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "ts_x, ts_y = data_reshape(ts_dta,\n",
    "                          bool_target_seperate = True)\n",
    "# prediction\n",
    "path_res = \"../results/m1_t5_1/\"\n",
    "# py: [bayes_mean, bayes_total_var, bayes_vola, bayes_unc, bayes_gate_src, bayes_gate_src_var, g_src_sample]\n",
    "py = pickle.load(open(path_res + \"py_\" + dataset + \"_bayes_one\" + \".p\", \"rb\"), \n",
    "                 encoding='latin1')\n",
    "\n",
    "# src_dict = {0:\"Bitfinex trans.\",\n",
    "#             1:\"Bitstamp trans.\", \n",
    "#             2:\"Bitfinex order book\",\n",
    "#             3:\"Bitstamp order book\", \n",
    "#             4:\"commom pattern\"}\n",
    "# plot_one(py,\n",
    "#          y = ts_y,\n",
    "#          plot_l = 800,\n",
    "#          plot_r = 950,\n",
    "#          path_fig = \"../figure_volume/m1_t5\",\n",
    "#          src_dict = src_dict,\n",
    "#          yrange = [0, 500], \n",
    "#          market_id = 1)\n",
    "\n",
    "rmse_mae_quantiles(target = ts_y, pred = py[0], q = 4)\n",
    "\n",
    "rel_rmse_mae_quantiles(target = np.squeeze(ts_y), \n",
    "                       pred = py[0],\n",
    "                       q=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Market 2 target 5min\n",
    "\n",
    "# ground truth data\n",
    "dataset = 'market2_tar5_len10'\n",
    "path_data = \"../datasets/bitcoin/market2_tar5_len10/\"\n",
    "ts_dta = pickle.load(open(path_data + 'test.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "ts_x, ts_y = data_reshape(ts_dta,\n",
    "                          bool_target_seperate = True)\n",
    "# prediction\n",
    "path_res = \"../results/m2_t5_1/\"\n",
    "# py: [bayes_mean, bayes_total_var, bayes_vola, bayes_unc, bayes_gate_src, bayes_gate_src_var, g_src_sample]\n",
    "py = pickle.load(open(path_res + \"py_\" + dataset + \"_bayes_one\" + \".p\", \"rb\"), \n",
    "                 encoding='latin1')\n",
    "\n",
    "# src_dict = {0:\"Bitfinex trans.\", \n",
    "#             1:\"Bitstamp trans.\", \n",
    "#             2:\"Bitfinex order book\", \n",
    "#             3:\"Bitstamp order book\", \n",
    "#             4:\"commom pattern\"}\n",
    "# plot_one(py,\n",
    "#          y = ts_y,\n",
    "#          plot_l = 50,\n",
    "#          plot_r = 200, \n",
    "#          path_fig = \"../figure_volume/m2_t5\", \n",
    "#          src_dict = src_dict, \n",
    "#          yrange = [0, 900], \n",
    "#          market_id = 2)\n",
    "\n",
    "rmse_mae_quantiles(target = ts_y, pred = py[0], q = 4)\n",
    "\n",
    "rel_rmse_mae_quantiles(target = np.squeeze(ts_y), \n",
    "                       pred = py[0],\n",
    "                       q=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Market 1 target 10min\n",
    "\n",
    "# ground truth data\n",
    "dataset = 'market1_tar10_len10'\n",
    "path_data = \"../datasets/bitcoin/market1_tar10_len10/\"\n",
    "ts_dta = pickle.load(open(path_data + 'test.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "ts_x, ts_y = data_reshape(ts_dta,\n",
    "                          bool_target_seperate = True)\n",
    "# prediction\n",
    "path_res = \"../results/m1_t10_1/\"\n",
    "# py: [bayes_mean, bayes_total_var, bayes_vola, bayes_unc, bayes_gate_src, bayes_gate_src_var, g_src_sample]\n",
    "py = pickle.load(open(path_res + \"py_\" + dataset + \"_top_one\" + \".p\", \"rb\"), \n",
    "                 encoding='latin1')\n",
    "\n",
    "# src_dict = {0:\"Bitfinex trans.\", \n",
    "#             1:\"Bitstamp trans.\", \n",
    "#             2:\"Bitfinex order book\", \n",
    "#             3:\"Bitstamp order book\", \n",
    "#             4:\"commom pattern\"}\n",
    "# plot_one(py,\n",
    "#          y = ts_y,\n",
    "#          plot_l = 1,\n",
    "#          plot_r = 150,\n",
    "#          path_fig = \"../figure_volume/m1_t10\",\n",
    "#          src_dict = src_dict,\n",
    "#          yrange = [0, 800], \n",
    "#          market_id = 1)\n",
    "\n",
    "rmse_mae_quantiles(target = ts_y, pred = py[0], q = 4)\n",
    "\n",
    "rel_rmse_mae_quantiles(target = np.squeeze(ts_y), \n",
    "                       pred = py[0],\n",
    "                       q=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Market 2 target 10min\n",
    "\n",
    "# ground truth data\n",
    "dataset = 'market2_tar10_len10'\n",
    "path_data = \"../datasets/bitcoin/market2_tar10_len10/\"\n",
    "ts_dta = pickle.load(open(path_data + 'test.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "ts_x, ts_y = data_reshape(ts_dta,\n",
    "                          bool_target_seperate = True)\n",
    "# prediction\n",
    "path_res = \"../results/m2_t10_1/\"\n",
    "# py: [bayes_mean, bayes_total_var, bayes_vola, bayes_unc, bayes_gate_src, bayes_gate_src_var, g_src_sample]\n",
    "py = pickle.load(open(path_res + \"py_\" + dataset + \"_top_one\" + \".p\", \"rb\"), \n",
    "                 encoding='latin1')\n",
    "\n",
    "# src_dict = {0:\"Bitfinex trans.\", \n",
    "#             1:\"Bitstamp trans.\", \n",
    "#             2:\"Bitfinex order book\",\n",
    "#             3:\"Bitstamp order book\", \n",
    "#             4:\"commom pattern\"}\n",
    "# plot_one(py,\n",
    "#          y = ts_y,\n",
    "#          plot_l = 150,\n",
    "#          plot_r = 300,\n",
    "#          path_fig = \"../figure_volume/m2_t10\",\n",
    "#          src_dict = src_dict,\n",
    "#          yrange = [0, 400], \n",
    "#          market_id = 2)\n",
    "\n",
    "rmse_mae_quantiles(target = ts_y, pred = py[0], q = 4)\n",
    "\n",
    "rel_rmse_mae_quantiles(target = np.squeeze(ts_y), \n",
    "                       pred = py[0],\n",
    "                       q=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ground truth data\n",
    "\n",
    "dataset = 'market2_tar5_len10'\n",
    "path_data = \"../datasets/bitcoin/market2_tar5_len10/\"\n",
    "\n",
    "dta = pickle.load(open(path_data + 'train.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "x, y = data_reshape(dta,\n",
    "                    bool_target_seperate = True)\n",
    "\n",
    "dta = pickle.load(open(path_data + 'val.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "x, y = data_reshape(dta,\n",
    "                    bool_target_seperate = True)\n",
    "\n",
    "dta = pickle.load(open(path_data + 'test.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "x, y = data_reshape(dta,\n",
    "                    bool_target_seperate = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ground truth data\n",
    "\n",
    "dataset = 'market2_tar1_len10'\n",
    "path_data = \"../datasets/bitcoin/market2_tar1_len10/\"\n",
    "\n",
    "dta = pickle.load(open(path_data + 'train.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "x, y = data_reshape(dta,\n",
    "                    bool_target_seperate = True)\n",
    "\n",
    "dta = pickle.load(open(path_data + 'val.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "x, y = data_reshape(dta,\n",
    "                    bool_target_seperate = True)\n",
    "\n",
    "dta = pickle.load(open(path_data + 'test.p', \"rb\"),\n",
    "                     encoding = 'latin1')\n",
    "x, y = data_reshape(dta,\n",
    "                    bool_target_seperate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "from scipy.optimize import fmin_slsqp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def func(p, r, xa, xb):\n",
    "    return truncnorm.nnlf(p, r)\n",
    "\n",
    "def constraint(p, r, xa, xb):\n",
    "    a, b, loc, scale = p\n",
    "    return np.array([a*scale + loc - xa, b*scale + loc - xb])\n",
    "\n",
    "xa, xb = 30, 250 \n",
    "loc = 50\n",
    "scale = 75\n",
    "\n",
    "a = (xa - loc)/scale\n",
    "b = (xb - loc)/scale\n",
    "\n",
    "# Generate some data to work with.\n",
    "r = truncnorm.rvs(a, b, loc=loc, scale=scale, size=10000)\n",
    "\n",
    "loc_guess = 30\n",
    "scale_guess = 90\n",
    "a_guess = (xa - loc_guess)/scale_guess\n",
    "b_guess = (xb - loc_guess)/scale_guess\n",
    "p0 = [a_guess, b_guess, loc_guess, scale_guess]\n",
    "\n",
    "par = fmin_slsqp(func, p0, f_eqcons=constraint, args=(r, xa, xb),\n",
    "                 iprint=False, iter=1000)\n",
    "\n",
    "xmin = 0\n",
    "xmax = 300\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x, truncnorm.pdf(x, a, b, loc=loc, scale=scale),\n",
    "        'r-', lw=3, alpha=0.4, label='truncnorm pdf')\n",
    "ax.plot(x, truncnorm.pdf(x, *par),\n",
    "        'k--', lw=1, alpha=1.0, label='truncnorm fit')\n",
    "ax.hist(r, bins=15, density=True, histtype='stepfilled', alpha=0.3)\n",
    "ax.legend(shadow=True)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_data = \"../dataset/bitcoin/market1_tar5_len10/\"\n",
    "\n",
    "tr_dta = pickle.load(open(path_data + 'train.p', \"rb\"), encoding = 'latin1')    \n",
    "tr_x, tr_y = data_reshape(tr_dta,\n",
    "                          bool_target_seperate = False)\n",
    "\n",
    "val_dta = pickle.load(open(path_data + 'val.p', \"rb\"), encoding = 'latin1')\n",
    "val_x, val_y = data_reshape(val_dta,\n",
    "                          bool_target_seperate = False)\n",
    "\n",
    "ts_dta = pickle.load(open(path_data + 'test.p', \"rb\"), encoding = 'latin1')\n",
    "ts_x, ts_y = data_reshape(ts_dta,\n",
    "                          bool_target_seperate = False)\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 3,figsize=(18,5));\n",
    "\n",
    "ax[0].hist(tr_y, 2000, label = 'Train', density = True);\n",
    "ax[0].legend()\n",
    "ax[0].set_xlim([-10, 500]);\n",
    "ax[0].set_ylim([0, 0.05]);\n",
    "\n",
    "ax[1].hist(val_y, 2000, label = 'Val.', density = True);\n",
    "ax[1].legend()\n",
    "ax[1].set_xlim([-10, 500]);\n",
    "ax[1].set_ylim([0, 0.05]);\n",
    "\n",
    "ax[2].hist(ts_y, 2000, label = 'Test', density = True);\n",
    "ax[2].legend()\n",
    "ax[2].set_xlim([-10, 500]);\n",
    "ax[2].set_ylim([0, 0.05]);\n",
    "\n",
    "# ax.set_xlim([-10, 500]);\n",
    "# ax.set_title(\"True values in training data\");\n",
    "\n",
    "# # fig, ax = plt.subplots();\n",
    "# ax.hist(val_y, 2000, label = 'Val.');\n",
    "# ax.set_xlim([-10, 500]);\n",
    "# ax.set_title(\"True values in validation data\");\n",
    "\n",
    "\n",
    "# # fig, ax = plt.subplots();\n",
    "# ax.hist(ts_y, 2000, label = 'Test');\n",
    "# ax.set_xlim([-10, 500]);\n",
    "# ax.set_title(\"True values in testing data\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  hyper_para generator \n",
    "\n",
    "from utils_training import *\n",
    "\n",
    "para_hpara_range = [[0.001, 0.001], [16, 80], [1e-7, 0.1]]\n",
    "para_hpara_n_trial = 15\n",
    "\n",
    "hpara_generator = hyper_para_random_search(para_hpara_range, \n",
    "                                              para_hpara_n_trial)\n",
    "\n",
    "tmp_hpara = hpara_generator.hpara_trial()\n",
    "cnt = 0\n",
    "\n",
    "while tmp_hpara != None:\n",
    "    \n",
    "    print(tmp_hpara)\n",
    "    \n",
    "    tmp_hpara = hpara_generator.hpara_trial()\n",
    "    \n",
    "    cnt += 1\n",
    "    \n",
    "print(cnt)\n",
    "\n",
    "# ------ \n",
    "\n",
    "from utils_training import *\n",
    "\n",
    "\n",
    "para_lr_range = [0.001, ]\n",
    "para_batch_range = [64, 32, 16, 80]\n",
    "para_l2_range = [1e-7, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "hpara_generator = hpara_grid_search([para_lr_range, para_batch_range, para_l2_range])\n",
    "\n",
    "tmp_hpara = hpara_generator.hpara_trial()\n",
    "cnt = 0\n",
    "\n",
    "while tmp_hpara != None:\n",
    "    \n",
    "    print(tmp_hpara)\n",
    "    \n",
    "    tmp_hpara = hpara_generator.hpara_trial()\n",
    "    \n",
    "    cnt += 1\n",
    "    \n",
    "print(cnt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mixture_gaussian = (norm.pdf(x_axis, -3, 1) + norm.pdf(x_axis, 3, 1)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "n = 10000 # number of sample to be drawn\n",
    "mu = [-6, 5]\n",
    "sigma = [2, 3]\n",
    "samples = []\n",
    "\n",
    "for i in range(n): # iteratively draw samples\n",
    "    Z = np.random.choice([0,1]) # latent variable\n",
    "    samples.append(np.random.normal(mu[Z], sigma[Z], 1))\n",
    "    \n",
    "sns.distplot(samples, hist=False)\n",
    "plt.show()\n",
    "sns.distplot(samples)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "distributions = [\n",
    "    {\"type\": np.random.lognormal, \"kwargs\": {\"mean\": -0.35, \"sigma\": 0.2}},\n",
    "]\n",
    "\n",
    "coefficients = np.array([1.0])\n",
    "coefficients /= coefficients.sum()      # in case these did not add up to 1\n",
    "sample_size = 100000\n",
    "\n",
    "num_distr = len(distributions)\n",
    "data = np.zeros((sample_size, num_distr))\n",
    "for idx, distr in enumerate(distributions):\n",
    "    data[:, idx] = distr[\"type\"](size=(sample_size,), **distr[\"kwargs\"])\n",
    "    \n",
    "random_idx = np.random.choice(np.arange(num_distr), size = (sample_size,), p = coefficients)\n",
    "sample = data[np.arange(sample_size), random_idx]\n",
    "plt.hist(sample, bins=100, density=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "distributions = [\n",
    "    {\"type\": np.random.lognormal, \"kwargs\": {\"mean\": -0.35, \"sigma\": 0.2}},\n",
    "    {\"type\": np.random.lognormal, \"kwargs\": {\"mean\": -0.1, \"sigma\": 0.2}},\n",
    "    {\"type\": np.random.lognormal, \"kwargs\": {\"mean\": -0.002, \"sigma\": 0.1}},\n",
    "]\n",
    "\n",
    "coefficients = np.array([0.5, 0.2, 0.3])\n",
    "coefficients /= coefficients.sum()      # in case these did not add up to 1\n",
    "sample_size = 100000\n",
    "\n",
    "num_distr = len(distributions)\n",
    "data = np.zeros((sample_size, num_distr))\n",
    "for idx, distr in enumerate(distributions):\n",
    "    data[:, idx] = distr[\"type\"](size=(sample_size,), **distr[\"kwargs\"])\n",
    "    \n",
    "random_idx = np.random.choice(np.arange(num_distr), size = (sample_size,), p = coefficients)\n",
    "sample = data[np.arange(sample_size), random_idx]\n",
    "plt.hist(sample, bins=100, density=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "distributions = [\n",
    "    {\"type\": np.random.lognormal, \"kwargs\": {\"mean\": -0.35, \"sigma\": 0.2}},\n",
    "    {\"type\": np.random.lognormal, \"kwargs\": {\"mean\": -0.1, \"sigma\": 0.2}},\n",
    "    {\"type\": np.random.lognormal, \"kwargs\": {\"mean\": -0.002, \"sigma\": 0.1}},\n",
    "]\n",
    "\n",
    "coefficients = np.array([0.1, 0.6, 0.3])\n",
    "coefficients /= coefficients.sum()      # in case these did not add up to 1\n",
    "sample_size = 100000\n",
    "\n",
    "num_distr = len(distributions)\n",
    "data = np.zeros((sample_size, num_distr))\n",
    "for idx, distr in enumerate(distributions):\n",
    "    data[:, idx] = distr[\"type\"](size=(sample_size,), **distr[\"kwargs\"])\n",
    "    \n",
    "random_idx = np.random.choice(np.arange(num_distr), size = (sample_size,), p = coefficients)\n",
    "sample = data[np.arange(sample_size), random_idx]\n",
    "plt.hist(sample, bins=100, density=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
