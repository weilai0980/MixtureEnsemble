--------------- FINISHED -------------------

statistic mixture 

gate logit central regularization

no dependency on hiddens
markov for statistic version

ELBO
  
validation metric
cut the data on one time step

Bayesian regression

server config 

smoothing or adaptive 

*speedup version 

regularization on the latent dependence
variance squared function 

lower_bound for pos_neg_diff_sq

version merge and cross check 

*constant terms 
  mean
  variance

*performance monitoring:
  py_mean_src
  py_var_src
  
loss: homoscedasticity-mse, heteroscedastic-lk_inv

Logging: 
  py_pikcle
  epoch_log
  
  
l2 hyper-para on latent smooth 
 
 yes
 
    same scale l2
    
       gate regu or not 
  
    lower scale l2
    
       gate regu or not 
 
 *no
    same scale l2
    
       gate regu or not 
  
    lower scale l2
    
       gate regu or not 

  
*testing output
   py_mean
   py_var
   py_mean_src
   py_var_src
   
bias term on gate function?


Predictive Uncertainty Estimation via Prior Networks

Uncertainty-Aware Learning from Demonstration Using Mixture Density Networks with Sampling-Free Variance Modeling

Uncertainty-Aware Attention for Reliable Interpretation and Prediction


dropout on bilinear transformation
performance optimizatin issue
  l2 on latent dependence faster?
  

----- constant variance MSE  

-- ml on single source
-- ml on multi source 
   



--------------- POINT LEARNED -------------------

bias term effect on generalization ability

lk_inv is numerical stable



--------------- TO DO -------------------

---- Boosting

---- Diagnostic 


---- Tuning

https://karpathy.github.io/2019/04/25/recipe/?utm_source=Deep+Learning+Weekly&utm_campaign=6049a04f13-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_term=0_384567b42d-6049a04f13-72960733


https://blog.nanonets.com/hyperparameter-optimization/?utm_campaign=Deep%20Learning%20Weekly&utm_medium=email&utm_source=Revue%20newsletter


----- TRAINING HYPER-PARA:

  train mode:  mse, mle, map, ELBO, EM, bayesian 


----- DATA
data inspection

correlated variable elimination


----- survey


----- experiment plan


----- Statistic heteroskedasticity mixture

*math operation check
   
Regularization:

  latent dependence
  l1
  fine-grained regularization hyper-para 

Training:
  learning rate decay
  grid search / random search 
  stop training on steps, instead of epoch 

*testing ensemble


EM training
inference 
output wrapper positive


----- Neural heteroskedasticity mixture

hidden transition
  moving averaging for neural network version



----- bayesian

concrete dropout

mc-dropout
SG-VI
SG-MCMC
  SG-LD
   
Random forest 
   http://contrib.scikit-learn.org/forest-confidence-interval/installation_guide.html


----- baselines

  regression, constant terms
  
  lstm + att
  cnn + lstm
  dual rnn
  
  Gradient boosted tree
  LightGBM
  Random forest
   
  XGboosted
  
  sub-sample in gradient boosted tree 

  further check of ARCH series 
  
  
  
  
