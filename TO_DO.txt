--------------- FINISHED -------------------

statistic mixture 

gate logit central regularization

no dependency on hiddens
markov for statistic version

ELBO
  
validation metric
cut the data on one time step

Bayesian regression

server config 

smoothing or adaptive 

*speedup version 

regularization on the latent dependence
variance squared function 

lower_bound for pos_neg_diff_sq

version merge and cross check 

*constant terms 
  mean
  variance

*performance monitoring:
  py_mean_src
  py_var_src
  
loss: homoscedasticity-mse, heteroscedastic-lk_inv

Logging: 
  py_pikcle
  epoch_log
  
  
l2 hyper-para on latent smooth 
 
 yes
 
    same scale l2
    
       gate regu or not 
  
    lower scale l2
    
       gate regu or not 
 
 *no
    same scale l2
    
       gate regu or not 
  
    lower scale l2
    
       gate regu or not 

  
*testing output
   py_mean
   py_var
   py_mean_src
   py_var_src
   
bias term on gate function?


Predictive Uncertainty Estimation via Prior Networks

Uncertainty-Aware Learning from Demonstration Using Mixture Density Networks with Sampling-Free Variance Modeling

Uncertainty-Aware Attention for Reliable Interpretation and Prediction


dropout on bilinear transformation
performance optimizatin issue
  l2 on latent dependence faster?
  

--- constant variance MSE  

-- ml on single source
-- ml on multi source 
   

** learning rate decay

 / random search 


** l2, # epoches in random search 

** grid search


training-testing framework
average consistency

early stopping: epoch-wise,

Testing:
  ensemble

bias terms for different data sources 
  bias as a constant effect




--------------- POINT LEARNED -------------------


Model:

   bias term effect on generalization ability. It is related to the marginal mean of the target. 
   lk_inv is numerical stable

   ensemble
     mixture
     average
     weighted sum/ linear combination
     

Training-validation:
   
  hyper-para search: grid random bayesian 

  early stopping using change-point detection for generalization ability

  set of optimal hyper-parameters
  snapshots under one certain hyper-parameter
  
  discrepency between validation and testing data
  
  hyper-para:
    learning rate
    regularization
    batch-size
    epoch 
    

Testing:

  validation and testing data difference
  

** Design choice:
  
  lk -> lk_inv -> 
  l2 on latent -> 
  
  lr decay in Bayesian setting
  
  random or grid? -> 

  constant, scalar, vec, pos_neg
  
                 
  bias: mean    variance   gate
  l2: mean    variance   gate
  gate logits:


--------------- TO DO -------------------


---- Tuning


https://karpathy.github.io/2019/04/25/recipe/?utm_source=Deep+Learning+Weekly&utm_campaign=6049a04f13-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_term=0_384567b42d-6049a04f13-72960733


https://blog.nanonets.com/hyperparameter-optimization/?utm_campaign=Deep%20Learning%20Weekly&utm_medium=email&utm_source=Revue%20newsletter


https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21



----- DATA

data inspection

correlated variable elimination

*remove auto-regressive part



----- Survey

domain transfer learning

multi-task

meta-learning 

AutoML 


----- Statistic heteroskedasticity mixture


Data:
  
  data augments: spikes
  
  discrepency between validation and testing data
  
  ** DATA augmentation
     outlier
     lag effect 



Model:

  
  *** Versatility
  
  feature interaction component 
  
  generic feature extractor on time series: 
    
    pre-augment
       mean
       variance
       deseasonalized 
     
    end2end augment
  

  ** boosting jump component/ statistical boosting
     integrated learning
     boosting learning, burn-in phase
     
     model snapshot ensemble 
     
  
  ** Neural version
     
     normalization:
       batch
       layer 
       weight normalization
       
  
     hidden transition
     moving averaging for neural network version
  
  
  loss decomposition 
  
  
  imbalanced target values:
    data augmentation by sampling
    boosting
    weighted loss function
    

Training:

  training error 

  boosting
  data augments: spikes
  
  early stopping: batch-wise, change point detection, exponential moving average 
  
  Cyclical learning rates
  
  EM training
  inference 
  output wrapper positive
  
  train mode:  mse-mix, mse-dense, map, ELBO, EM, bayesian 
  
  
  ** bayeisan SG-MCMC:
  
     asymptotic analysis
     convergence analysis
  
     epoch-wise or batch-wise
  
     different optimizers
     thinning
     ancestral sampling
     simulated anealing
     
     heteroskedasticity and MCMC inference
     math operation check
     
     
     
  Regularization:
  
    l1
    fine-grained regularization hyper-para
    
    latent dependence     
    imbalanced regularization on mean and variance
    
     

Testing:

  discrepency between validation and testing data
  
  ** supervised jump detection 


Monitoring and diagnostic:


Inference:
  
  remove outliers 
  
  ** truncated Gaussian on source contribution uncertainty




----- bayesian


Bayesian:
  
  MCMC
    SG-MCMC: SG-LD, SG-HMC
    
  VI 
    MC-Dropout
    Posterior-sharpening
    
  SGWA


Variational inference and sampling methods:

  REINFORCE from gradient
  Importance Sampling from gradient
  
  Re-parameteraization Trick 
  Monte Carlo
  
  
Random forest 
  http://contrib.scikit-learn.org/forest-confidence-interval/installation_guide.html



----- baselines


  stochvol and fGarch
  
  regression, constant terms
  
  lstm + att
  cnn + lstm
  dual rnn
  
  Gradient boosted tree
  LightGBM
  Random forest
   
  XGboosted
  
  sub-sample in gradient boosted tree 

  further check of ARCH series 
  
  
  
  