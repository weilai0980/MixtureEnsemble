--------------- FINISHED -------------------

statistic mixture 

gate logit central regularization

no dependency on hiddens
markov for statistic version

ELBO
  
validation metric
cut the data on one time step

Bayesian regression

server config 

smoothing or adaptive 

*speedup version 

regularization on the latent dependence
variance squared function 

lower_bound for pos_neg_diff_sq

version merge and cross check 

*constant terms 
  mean
  variance

*performance monitoring:
  py_mean_src
  py_var_src
  
loss: homoscedasticity-mse, heteroscedastic-lk_inv

Logging: 
  py_pikcle
  epoch_log
  
  
l2 hyper-para on latent smooth 
 
 yes
 
    same scale l2
    
       gate regu or not 
  
    lower scale l2
    
       gate regu or not 
 
 *no
    same scale l2
    
       gate regu or not 
  
    lower scale l2
    
       gate regu or not 

  
*testing output
   py_mean
   py_var
   py_mean_src
   py_var_src
   
bias term on gate function?


Predictive Uncertainty Estimation via Prior Networks

Uncertainty-Aware Learning from Demonstration Using Mixture Density Networks with Sampling-Free Variance Modeling

Uncertainty-Aware Attention for Reliable Interpretation and Prediction


dropout on bilinear transformation
performance optimizatin issue
  l2 on latent dependence faster?
  

----- constant variance MSE  

-- ml on single source
-- ml on multi source 
   

** learning rate decay

 / random search 


** l2, # epoches in random search 

** grid search


training-testing framework
average consistency

early stopping: epoch-wise,

Testing:
  ensemble

bias terms for different data sources 
  bias as a constant effect


--------------- POINT LEARNED -------------------


Model:

   bias term effect on generalization ability. It is related to the marginal mean of the target. 
   lk_inv is numerical stable

Training-validation:
   
  hyper-para search: grid random bayesian 

  early stopping using change-point detection for generalization ability

  set of optimal hyper-parameters
  snapshots under one certain hyper-parameter
  
  discrepency between validation and testing data

Testing:

  validation and testing data difference
  

design choice:
  lk -> lk_inv -> l2 on latent -> lr decay -> random or grid? -> constant, scalar, vec, pos_neg


--------------- TO DO -------------------

---- Boosting


---- Diagnostic 


---- Tuning


https://karpathy.github.io/2019/04/25/recipe/?utm_source=Deep+Learning+Weekly&utm_campaign=6049a04f13-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_term=0_384567b42d-6049a04f13-72960733


https://blog.nanonets.com/hyperparameter-optimization/?utm_campaign=Deep%20Learning%20Weekly&utm_medium=email&utm_source=Revue%20newsletter


https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21


----- TRAINING HYPER-PARA:

  train mode:  mse, mle, map, ELBO, EM, bayesian 


----- DATA

data inspection

correlated variable elimination


----- survey


----- experiment plan


----- Statistic heteroskedasticity mixture


* heteroskedasticity and MCMC inference


*math operation check


Regularization:

  
  latent dependence
  l1
  fine-grained regularization hyper-para 
  
  
  imbalanced regularization on mean and variance
   

Data:
  
  data augments: spikes
  
  discrepency between validation and testing data


Training:

  early stopping: batch-wise, change point detection, exponential moving average 
  
  Cyclical learning rates
     
     
Cross-check:

  ** lk > lk_inv > l2 on latent > lr decay > random or grid? > constant, scalar, vec, pos_neg?


** SG-MCMC:

   thinning


Testing:

  discrepency between validation and testing data 


EM training
inference 
output wrapper positive




----- Neural heteroskedasticity mixture

hidden transition
  moving averaging for neural network version



----- bayesian


sgld efficiency


concrete dropout

mc-dropout
SG-VI


SG-MCMC
  SG-LD
   
Random forest 
   http://contrib.scikit-learn.org/forest-confidence-interval/installation_guide.html


----- baselines

  regression, constant terms
  
  lstm + att
  cnn + lstm
  dual rnn
  
  Gradient boosted tree
  LightGBM
  Random forest
   
  XGboosted
  
  sub-sample in gradient boosted tree 

  further check of ARCH series 
  
  
  
  
