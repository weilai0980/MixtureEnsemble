--------------- FINISHED -------------------

statistic mixture 

gate logit central regularization

no dependency on hiddens
markov for statistic version

ELBO
  
validation metric
cut the data on one time step

Bayesian regression


--------------- TO DO -------------------


----- TRAINING HYPER-PARA:

  loss: homoscedasticity-mse, heteroscedastic-lk_inv
  train mode: bayesian, map, EM 


----- DATA
data inspection


----- baseline

sub-sample in gradient boosted tree 

further check of ARCH series 


----- survey

Predictive Uncertainty Estimation via Prior Networks

Uncertainty-Aware Learning from Demonstration Using Mixture Density Networks with Sampling-Free Variance Modeling

Uncertainty-Aware Attention for Reliable Interpretation and Prediction


----- constant variance MSE  

-- ml on single source
-- ml on multi source 
   
   Gradient boosted tree
   LightGBM
   Random forest
   
   XGboosted
   
----- heteroskedasticity

server config 

*hidden transition
  moving averaging for neural network version 

*Regularization:
  dropout on bilinear transformation 
  output wrapper positive

*EM training
*inference 


neural mixture

  cnn + lstm
  dual rnn

  lstm + att lk


----- bayesian

concrete dropout
SG-VI 
SG-LD
SG-MCMC

mc-dropout
   
Random forest 
   http://contrib.scikit-learn.org/forest-confidence-interval/installation_guide.html

