--------------- FINISHED -------------------

statistic mixture 

gate logit central regularization

no dependency on hiddens
markov for statistic version

ELBO
  
validation metric
cut the data on one time step

Bayesian regression

server config 

smoothing or adaptive 

*speedup version 

regularization on the latent dependence
variance squared function 

lower_bound for pos_neg_diff_sq

version merge and cross check 


--------------- TO DO -------------------


----- TRAINING HYPER-PARA:

  loss: homoscedasticity-mse, heteroscedastic-lk_inv
  train mode: bayesian, map, EM 


----- DATA
data inspection

correlated variable elimination


----- baseline

sub-sample in gradient boosted tree 

further check of ARCH series 


----- survey

Predictive Uncertainty Estimation via Prior Networks

Uncertainty-Aware Learning from Demonstration Using Mixture Density Networks with Sampling-Free Variance Modeling

Uncertainty-Aware Attention for Reliable Interpretation and Prediction


----- constant variance MSE  

-- ml on single source
-- ml on multi source 
   
   Gradient boosted tree
   LightGBM
   Random forest
   
   XGboosted
   

----- experiment plan

l2 hyper-para on latent smooth 
 
 yes
 
    same scale l2
    
       gate regu or not 
  
    lower scale l2
    
       gate regu or not 
 
 *no
    same scale l2
    
       gate regu or not 
  
    lower scale l2
    
       gate regu or not 


----- heteroskedasticity

hidden transition
  moving averaging for neural network version
   
Regularization:
  dropout on bilinear transformation
  l1 normalization
  
output wrapper positive

EM training
inference 


Logging: 
  py_pikcle
  epoch_log




*constant terms 

testing ensemble 

performance optimizatin issue
  l2 on latent dependence faster?

Neural Mixture




----- bayesian

concrete dropout

SG-VI: mc-dropout
SG-LD
SG-MCMC
   
Random forest 
   http://contrib.scikit-learn.org/forest-confidence-interval/installation_guide.html


----- baselines

  lstm + att
  cnn + lstm
  dual rnn
  
  
  
  
