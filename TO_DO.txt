--------------- FINISHED -------------------

statistic mixture 

gate logit central regularization

no dependency on hiddens
markov for statistic version

ELBO
  
validation metric
cut the data on one time step

Bayesian regression

server config 

smoothing or adaptive 

*speedup version 

regularization on the latent dependence
variance squared function 

lower_bound for pos_neg_diff_sq

version merge and cross check 

*constant terms 
  mean
  variance

*performance monitoring:
  py_mean_src
  py_var_src
  
loss: homoscedasticity-mse, heteroscedastic-lk_inv

Logging: 
  py_pikcle
  epoch_log
  
  
l2 hyper-para on latent smooth 
 
 yes
 
    same scale l2
    
       gate regu or not 
  
    lower scale l2
    
       gate regu or not 
 
 *no
    same scale l2
    
       gate regu or not 
  
    lower scale l2
    
       gate regu or not 

  
*testing output
   py_mean
   py_var
   py_mean_src
   py_var_src
   
bias term on gate function?


Predictive Uncertainty Estimation via Prior Networks

Uncertainty-Aware Learning from Demonstration Using Mixture Density Networks with Sampling-Free Variance Modeling

Uncertainty-Aware Attention for Reliable Interpretation and Prediction


dropout on bilinear transformation
performance optimizatin issue
  l2 on latent dependence faster?
  

--- constant variance MSE  

-- ml on single source
-- ml on multi source 
   

** learning rate decay

 / random search 


** l2, # epoches in random search 

** grid search

training-testing framework
average consistency

early stopping: epoch-wise,

Testing:
  ensemble

bias terms for different data sources 
  bias as a constant effect


     model snapshot ensemble 
     
     hidden transition
     moving averaging for neural network version
  
  

  training error 

  boosting
  
  early stopping: batch-wise, change point detection, exponential moving average 
  
  
  inference 
  output wrapper positive
  
  train mode:  mse-mix, mse-dense, map, ELBO, EM, bayesian 
  
  
  ** bayeisan SG-MCMC:
  
     
     heteroskedasticity and MCMC inference
     math operation check
     
     
  Regularization:
  
    fine-grained regularization hyper-para
    
    latent dependence     
    imbalanced regularization on mean and variance
    

Testing:

  discrepency between validation and testing data
  

model snapshot ensemble 

fine-grained regularization hyper-para
    
    latent dependence     
    imbalanced regularization on mean and variance


training error 
  early stopping: batch-wise, change point detection, exponential moving average
  epoch-wise or batch-wise
  different optimizers
  heteroskedasticity and MCMC inference
  discrepency between validation and testing data
  
density average

Bayesian:
  epoch/batch level
  random seed level model averaging
  hyper-para level
  
  one shot one retrainone-shot multi retraintop shots, multi retraintop shots one retrain
Bayes shots one retrainBayes shots multi retrain

error metric: correlation, 

global top shots  
like predictive interval coverage
--------------- POINT LEARNED -------------------

Model:

   bias term effect on generalization ability. It is related to the marginal mean of the target. 
   lk_inv is numerical stable

   ensemble
     mixture
     average
     weighted sum/ linear combination
     
Training-validation:
   
  hyper-para search: grid random bayesian 

  early stopping using change-point detection for generalization ability

  set of optimal hyper-parameters
  
  snapshots under one certain hyper-parameter
  
  discrepency between validation and testing data
  
  regularization:
    l2, 
    dropout
    network size 
  
  hyper-para search:
    
    learning rate
    regularization
    batch-size
    epoch     
  
  evaluation metric: 
    rmse, mae, mape, nnllk
    
    nnllk leads to generalization on all metrics
  
Testing:

  validation and testing data difference
  
Interpretation of uncertainty and volatility:

   uncertainty: rareness of x 
   
   uncertainty high, volatility high
   
   uncertainty high, volatility low
   
   uncertainty low, volatility high -> jump
   
   uncertainty low, volatility low
   
Bayesian:
  
  Monte Carlo
    importance sampling
  
  MCMC
    
    Gibs, Metropolis-hasting
    
    SG-MCMC: SG-LD, SG-HMC
    
  VI
    
    black-box VI 
    
    MC-Dropout
    Posterior-sharpening
    
  SGWA

Variational inference and sampling methods:

  REINFORCE from gradient
  Importance Sampling from gradient
  
  Re-parameteraization Trick 
  Monte Carlo
  
** Design choice:
  
  lk -> lk_inv -> 
  l2 on latent -> 
  
  lr decay in Bayesian setting
  
  random or grid? -> 

  constant, scalar, vec, pos_neg
                 
  bias: mean    variance   gate
  l2: mean    variance   gate
  gate logits:


** Tuning logic:

  network structure related to the representation ability, lr + batch_size, small epoch -> stable training/validation curve 
                                                                batch_size starts from large values for training stability
                                                                
  add regularization, e.g. l2 and/or dropout
  increas epoch as well as adding training heuristic, e.g. lr decay, lr warm-up, early stop, etc, 
  
  observe the best hyper-para and best epoch in the space of hyper-parameters
  adjust the space or (learning rate OR network size OR epochs)
  

--------------- TO DO -------------------


---- Tuning

https://karpathy.github.io/2019/04/25/recipe/?utm_source=Deep+Learning+Weekly&utm_campaign=6049a04f13-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_term=0_384567b42d-6049a04f13-72960733

https://blog.nanonets.com/hyperparameter-optimization/?utm_campaign=Deep%20Learning%20Weekly&utm_medium=email&utm_source=Revue%20newsletter

https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21

*check linear version: adam, sgmcmc

*rnn version: sgmcmc

----- DATA

data inspection

correlated variable elimination

*remove auto-regressive part

kaggle
  ASHRAE great energy predictor III


----- Survey

domain transfer learning

multi-task

meta-learning 

AutoML 

----- Statistic heteroskedasticity mixture

Data:
  
  data augments: spikes
  
  discrepency between validation and testing data
  
  ** DATA augmentation
     outlier
     lag effect

Model:

  posterior for inference
  temporal dependence
  
  mlp layer scheduler:
     2*input_size + 1
     2/3 input_size +/- output_size
     < 2*input_size
     
  ** residual layers
  
  feature interaction component 
  
  generic feature extractor on time series: 
    
    pre-augment
       mean
       variance
       deseasonalized 
     
    end2end augment

  ** boosting jump component/ statistical boosting
     integrated learning
     boosting learning, burn-in phase
  
  ** Neural version
     
     normalization:
       batch
       layer 
       weight normalization
       
  
     hidden transition
     moving averaging for neural network version
     
  *** calibration metric:
      refer to "on mixup training: improved calibration and predictive uncertainty" 
  *** temporal dependence
  *** sg mcmc
  
  loss decomposition 
  
  imbalanced target values:
    data augmentation by sampling
    boosting
    weighted loss function
    
Training:

  source-specific hyper-param
  monitor： number of instances
  error metric: interval coverage

  boosting
  data augments: spikes
  
  Cyclical learning rates
  
  EM training
  inference 
  output wrapper positive
  
  train mode:  mse-mix, mse-dense, map, ELBO, EM, bayesian 
  
  ** bayeisan SG-MCMC:
  
     asymptotic analysis
     convergence analysis
  
     ** Sampling:
          thinning
          ancestral sampling
          simulated anealing
     
     math operation check
     
  ** Add metrics "within confidence interval"   
     
  Regularization:
  
  l1
  
  Optimization:
    RAdam
    LookAhead
    AMSgrad 
    
  ** every N epoch to sample 
  
  epoch/batch ensembling
    difference between training and validation 

Testing:

  ** supervised jump detection 
     extream value theory KDD paper

Monitoring and diagnostic:

Inference:
  
  remove outliers 
  
  ** truncated Gaussian on source contribution uncertainty
  
  kernel density inference 

Bayesian:

Boosting:
  
  boosing bayesian models

----- baselines

  Random forest 
  http://contrib.scikit-learn.org/forest-confidence-interval/installation_guide.html

  regression, constant terms
  
  lstm + att: deep AR
  cnn + lstm: 
  dual rnn
  
  Gradient boosted tree
    sub-sample in gradient boosted tree 
    
  Random forest
  
  LightGBM
  XGboost
   
  ARMA_GARCH
  
  ARMAX_GARCH
  
  ARMAX_GARCHX  
  
  stochvol and fGarch
  