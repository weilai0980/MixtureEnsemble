{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "from random import shuffle\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# local packages \n",
    "from utils_libs import *\n",
    "from utils_training import *\n",
    "# from utils_inference import *\n",
    "# from mixture_models import *\n",
    "from pipeline import *\n",
    "\n",
    "# ------ GPU environment\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# ------ set-up\n",
    "para_train = {}\n",
    "\n",
    "# -- path\n",
    "para_train['arg_py'] = \"market1_tar10_len10\"\n",
    "para_train['path_data'] = \"../../datasets/bitcoin/market1_tar10_len10/\"\n",
    "para_train['path_model'] = \"../../results/volume/m1_t10_1/\"\n",
    "para_train['path_py'] = \"../../results/volume/\" + para_train['arg_py']\n",
    "para_train['path_log_error'] = \"../../results/volume/log_\" + para_train['arg_py'] + \".txt\"\n",
    "\n",
    "# -- data\n",
    "para_train['para_num_source'] = 4\n",
    "\n",
    "# -- model\n",
    "para_train['para_distr_type'] = 'log_normal_logOpt_linearComb' # \"log_normal_logOpt_linearComb\", 'normal'\n",
    "para_train['para_var_type'] = \"exp\" # square, exp\n",
    "para_train['para_share_type_gate'] = \"no_share\" # no_share, share, mix\n",
    "para_train['para_model_type'] = 'linear'\n",
    "\n",
    "para_train['para_bool_bias_in_mean'] = True\n",
    "para_train['para_bool_bias_in_var'] = True\n",
    "para_train['para_bool_bias_in_gate'] = True\n",
    "\n",
    "if para_train['para_model_type'] == 'rnn':\n",
    "    para_train['para_x_src_padding'] = False\n",
    "    para_train['para_add_common_factor'] = False\n",
    "    para_train['para_common_factor_type'] = \"pool\" if para_train['para_add_common_factor'] == True else \"\"\n",
    "    \n",
    "elif para_train['para_model_type'] == 'linear':\n",
    "    para_train['para_x_src_padding'] = True\n",
    "    para_train['para_add_common_factor'] = False\n",
    "    para_train['para_common_factor_type'] = \"factor\" if para_train['para_add_common_factor'] == True else \"\"\n",
    "\n",
    "para_train['para_bool_target_seperate'] = False # [Note] if yes, the last source corresponds to the auto-regressive target variable\n",
    "\n",
    "# -- training\n",
    "para_train['para_n_epoch'] = 90 # [Note] for sg_mcmc family, \"para_n_epoch\" could be set to higher values\n",
    "para_train['para_burn_in_epoch'] = 85\n",
    "para_train['para_vali_snapshot_num'] = max(1, int(0.05*para_train['para_n_epoch']))\n",
    "para_train['para_test_snapshot_num'] = 10\n",
    "\n",
    "para_train['para_hpara_search'] = \"random\" # random, grid \n",
    "para_train['para_hpara_train_trial_num'] = 30\n",
    "para_train['para_hpara_retrain_num'] = 10\n",
    "para_train['para_hpara_ensemble_trial_num'] = 3\n",
    "\n",
    "# -- optimization\n",
    "para_train['para_loss_type'] = \"heter_lk_inv\" # \"heter_lk_inv\"\n",
    "para_train['para_optimizer'] = \"adam\" # RMSprop, adam, sgd, adamW, sg_mcmc_RMSprop, sg_mcmc_adam\n",
    "para_train['para_optimizer_lr_decay_epoch'] = 10 # after the warm-up\n",
    "para_train['para_optimizer_lr_warmup_epoch'] = max(1, int(0.1*para_train['para_n_epoch']))\n",
    "\n",
    "para_train['para_early_stop_bool'] = False\n",
    "para_train['para_early_stop_window'] = 0\n",
    "\n",
    "para_train['para_validation_metric'] = 'nnllk'\n",
    "para_train['para_metric_map'] = {'rmse':0, 'mae':1, 'mape':2, 'nnllk':3}\n",
    "\n",
    "# -- regularization\n",
    "para_train['para_regu_mean'] = True\n",
    "para_train['para_regu_var'] = True\n",
    "para_train['para_regu_gate'] = False\n",
    "\n",
    "# ----- hpara: hyper parameter ranges\n",
    "\n",
    "para_hpara_range = {}\n",
    "para_hpara_range['random'] = {}\n",
    "para_hpara_range['random']['linear'] = {}\n",
    "para_hpara_range['random']['rnn'] = {}\n",
    "\n",
    "# -- linear\n",
    "if para_train['para_add_common_factor'] == True:\n",
    "    para_hpara_range['random']['linear']['factor_size'] = [10, 10]\n",
    "para_hpara_range['random']['linear']['lr'] = [1e-4, 5e-4]\n",
    "para_hpara_range['random']['linear']['batch_size'] = [10, 300]\n",
    "# source-wise\n",
    "# tar1\n",
    "# para_hpara_range['random']['linear']['l2_mean'] = [1e-1, 5e+1]\n",
    "# para_hpara_range['random']['linear']['l2_var']  = [1e-1, 5e+1]\n",
    "# # tar5\n",
    "# para_hpara_range['random']['linear']['l2_mean'] = [1e-1, 1e+1]\n",
    "# para_hpara_range['random']['linear']['l2_var']  = [1e-1, 1e+1]\n",
    "# tar10\n",
    "para_hpara_range['random']['linear']['l2_mean'] = [1e-1, 5e-0]\n",
    "para_hpara_range['random']['linear']['l2_var']  = [1e-1, 5e-0]\n",
    "\n",
    "# # -- rnn\n",
    "# # source-wise\n",
    "# para_hpara_range['random']['rnn']['rnn_size'] =  [16, 16]\n",
    "# para_hpara_range['random']['rnn']['dense_num'] = [0, 3] # inproper value leads to non-convergence in training\n",
    "\n",
    "# para_hpara_range['random']['rnn']['lr'] = [0.001, 0.001]\n",
    "# para_hpara_range['random']['rnn']['batch_size'] = [100, 140]\n",
    "\n",
    "# # source-wise\n",
    "# para_hpara_range['random']['rnn']['l2_mean'] = [1e-7, 1e-3]\n",
    "# para_hpara_range['random']['rnn']['l2_var'] = [1e-7, 1e-3]\n",
    "# if para_regu_gate == True:\n",
    "#     para_hpara_range['random']['linear']['l2_gate'] = [1e-7, 1e-3]\n",
    "    \n",
    "# para_hpara_range['random']['rnn']['dropout_keep_prob'] = [0.7, 1.0]\n",
    "# para_hpara_range['random']['rnn']['max_norm_cons'] = [0.0, 0.0]\n",
    "\n",
    "# ----- main process  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- data\n",
    "    src_tr_x, tr_y, src_val_x, val_y, src_ts_x, ts_y = prepare_data(para_train)\n",
    "    \n",
    "    # --- run the pipeline\n",
    "    log_setup(path = para_train['path_log_error'], \n",
    "              para_train = para_train,\n",
    "              para_hpara_range = para_hpara_range)\n",
    "    \n",
    "    print('\\n --- ', para_train)\n",
    "    \n",
    "    train_validate_test(src_tr_x,\n",
    "                        tr_y,\n",
    "                        src_val_x,\n",
    "                        val_y,\n",
    "                        src_ts_x,\n",
    "                        ts_y,\n",
    "                        hyper_para_range = para_hpara_range,\n",
    "                        para_train = para_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- hyper-para generator \n",
    "    if para_hpara_search == \"random\":        \n",
    "        hpara_generator = hyper_para_random_search(para_hpara_range[para_hpara_search][para_model_type], \n",
    "                                                   para_hpara_train_trial_num)\n",
    "    elif para_hpara_search == \"grid\":\n",
    "        hpara_generator = hyper_para_grid_search(para_hpara_range[para_hpara_search][para_model_type])\n",
    "            \n",
    "    # -- begin hyper-para search\n",
    "    hpara_log = []\n",
    "    \n",
    "    # sample one set-up of hyper-para\n",
    "    hpara_dict = hpara_generator.one_trial()\n",
    "                                                 \n",
    "    while hpara_dict != None:\n",
    "        \n",
    "        tr_dict = training_para_gen(shape_x_dict = shape_tr_x_dict, \n",
    "                                    hpara_dict = hpara_dict)\n",
    "        # hp_: hyper-parameter\n",
    "        # hp_step_error: [[step, train_metric, val_metric, epoch]]\n",
    "        hp_step_error, hp_epoch_time = training_validating(src_tr_x,\n",
    "                                                           tr_y,\n",
    "                                                           src_val_x,\n",
    "                                                           val_y,\n",
    "                                                           dim_x = para_dim_x,\n",
    "                                                           steps_x = para_steps_x,\n",
    "                                                           hyper_para_dict = hpara_dict,\n",
    "                                                           training_dict = tr_dict,\n",
    "                                                           retrain_bool = False,\n",
    "                                                           retrain_top_steps = [],\n",
    "                                                           retrain_bayes_steps = [],\n",
    "                                                           retrain_idx = 0,\n",
    "                                                           random_seed = 1)\n",
    "        \n",
    "        #[ dict{lr, batch, l2, ..., burn_in_steps}, [[step, tr_metric, val_metric, epoch]] ]\n",
    "        hpara_dict[\"burn_in_steps\"] = para_burn_in_epoch # tr_dict[\"batch_per_epoch\"] - 1\n",
    "        hpara_log.append([hpara_dict, hp_step_error])\n",
    "        \n",
    "        # -- prepare for the next trial\n",
    "        \n",
    "        # sample the next hyper-para\n",
    "        hpara_dict = hpara_generator.one_trial()\n",
    "        \n",
    "        # -- logging\n",
    "        log_train_val_performance(path_log_error,\n",
    "                                  hpara = hpara_log[-1][0],\n",
    "                                  hpara_error = hpara_log[-1][1][0],\n",
    "                                  train_time = hp_epoch_time)\n",
    "        # NAN loss exception\n",
    "        log_null_loss_exception(hp_step_error, \n",
    "                                path_log_error)\n",
    "        \n",
    "        print('\\n Validation performance under the hyper-parameters: \\n', hpara_log[-1][0], hpara_log[-1][1][0])\n",
    "        print('\\n Training time: \\n', hp_epoch_time, '\\n')\n",
    "        \n",
    "    # ------ re-train\n",
    "    #save all epoches in re-training, then select snapshots\n",
    "    \n",
    "    # best hyper-para\n",
    "    best_hpara, _, _, _, _ = hyper_para_selection(hpara_log, \n",
    "                                                  val_snapshot_num = para_vali_snapshot_num, \n",
    "                                                  test_snapshot_num = para_test_snapshot_num,\n",
    "                                                  metric_idx = para_metric_map[para_validation_metric])\n",
    "    retrain_hpara_steps = []\n",
    "    retrain_hpara_step_error = []\n",
    "    retrain_random_seeds = [1] + [randint(0, 1000) for _ in range(para_hpara_retrain_num-1)]\n",
    "    \n",
    "    for tmp_retrain_id in range(para_hpara_retrain_num):\n",
    "        \n",
    "        tr_dict = training_para_gen(shape_x_dict = shape_tr_x_dict,\n",
    "                                    hpara_dict = best_hpara)\n",
    "        \n",
    "        step_error, _ = training_validating(src_tr_x,\n",
    "                                            tr_y,\n",
    "                                            src_val_x,\n",
    "                                            val_y,\n",
    "                                            dim_x = para_dim_x,\n",
    "                                            steps_x = para_steps_x,\n",
    "                                            hyper_para_dict = best_hpara,\n",
    "                                            training_dict = tr_dict,\n",
    "                                            retrain_bool = True,\n",
    "                                            retrain_top_steps = list(range(para_n_epoch)), #top_steps,\n",
    "                                            retrain_bayes_steps = list(range(para_n_epoch)), #bayes_steps,\n",
    "                                            retrain_idx = tmp_retrain_id,\n",
    "                                            random_seed = retrain_random_seeds[tmp_retrain_id])\n",
    "        \n",
    "        top_steps, bayes_steps, top_steps_features, bayes_steps_features, val_error, step_error_pairs = snapshot_selection(train_log = step_error,\n",
    "                                                                                                                           snapshot_num = para_test_snapshot_num,\n",
    "                                                                                                                           total_step_num = para_n_epoch,\n",
    "                                                                                                                           metric_idx = para_metric_map[para_validation_metric],\n",
    "                                                                                                                           val_snapshot_num = para_vali_snapshot_num)\n",
    "        if len(top_steps) != 0:\n",
    "            retrain_hpara_steps.append([top_steps, bayes_steps, top_steps_features, bayes_steps_features, tmp_retrain_id, val_error])\n",
    "            retrain_hpara_step_error.append([step_error_pairs, tmp_retrain_id])\n",
    "        \n",
    "        log_val_hyper_para(path = path_log_error,\n",
    "                           hpara_tuple = [best_hpara, top_steps],\n",
    "                           error_tuple = step_error[0], \n",
    "                           log_string = \"-- \" + str(tmp_retrain_id))\n",
    "    \n",
    "        print('\\n----- Retrain hyper-parameters: ', best_hpara, top_steps, '\\n')\n",
    "        print('\\n----- Retrain validation performance: ', step_error[0], '\\n')\n",
    "    \n",
    "    sort_retrain_hpara_steps = sorted(retrain_hpara_steps, \n",
    "                                      key = lambda x:x[-1])\n",
    "    \n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [i[-2:] for i in sort_retrain_hpara_steps], \n",
    "                         ensemble_str = \"Retrain Ids and Vali. Errors: \")\n",
    "    \n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [i[-2:] for i in sort_retrain_hpara_steps[:para_hpara_ensemble_num]], \n",
    "                         ensemble_str = \"Retrain Ids for ensemble: \")\n",
    "    \n",
    "    # ------ testing\n",
    "    # error tuple: [rmse, mae, mape, nnllk]\n",
    "    # py_tuple\n",
    "    \n",
    "    # -- one snapshot from one retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [sort_retrain_hpara_steps[0][0][:1]],\n",
    "                                    retrain_ids = [ sort_retrain_hpara_steps[0][-2] ],\n",
    "                                    xts = src_ts_x, \n",
    "                                    yts = ts_y, \n",
    "                                    file_path = path_model, \n",
    "                                    bool_instance_eval = True,\n",
    "                                    loss_type = para_loss_type,\n",
    "                                    num_src = len(src_val_x),\n",
    "                                    snapshot_features = [],\n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [error_tuple], \n",
    "                         ensemble_str = \"One-shot-one-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_one_one\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- one snapshot from multi retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [tmp_steps[0][:1] for tmp_steps in sort_retrain_hpara_steps], \n",
    "                                    retrain_ids = [i[-2] for i in sort_retrain_hpara_steps[:para_hpara_ensemble_num]],\n",
    "                                    xts = src_ts_x,\n",
    "                                    yts = ts_y, \n",
    "                                    file_path = path_model,\n",
    "                                    bool_instance_eval = True,\n",
    "                                    loss_type = para_loss_type,\n",
    "                                    num_src = len(src_ts_x), \n",
    "                                    snapshot_features = [], \n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [error_tuple], \n",
    "                         ensemble_str = \"One-shot-multi-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_one_multi\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- top snapshots from one retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [sort_retrain_hpara_steps[0][0]], \n",
    "                                    retrain_ids = [ sort_retrain_hpara_steps[0][-2] ], \n",
    "                                    xts = src_ts_x, \n",
    "                                    yts = ts_y, \n",
    "                                    file_path = path_model,\n",
    "                                    bool_instance_eval = True, \n",
    "                                    loss_type = para_loss_type, \n",
    "                                    num_src = len(src_ts_x), \n",
    "                                    snapshot_features = [], \n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error,\n",
    "                         error_tuple = [error_tuple],\n",
    "                         ensemble_str = \"Top-shots-one-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_top_one\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- top snapshots multi retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [tmp_steps[0] for tmp_steps in sort_retrain_hpara_steps], \n",
    "                                    retrain_ids = [i[-2] for i in sort_retrain_hpara_steps[:para_hpara_ensemble_num]], \n",
    "                                    xts = src_ts_x,\n",
    "                                    yts = ts_y,\n",
    "                                    file_path = path_model,\n",
    "                                    bool_instance_eval = True,\n",
    "                                    loss_type = para_loss_type,\n",
    "                                    num_src = len(src_ts_x), \n",
    "                                    snapshot_features = [], \n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [error_tuple], \n",
    "                         ensemble_str = \"Top-shots-multi-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_top_multi\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- bayesian snapshots one retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [sort_retrain_hpara_steps[0][1]], \n",
    "                                    retrain_ids = [ sort_retrain_hpara_steps[0][-2] ], \n",
    "                                    xts = src_ts_x, \n",
    "                                    yts = ts_y,\n",
    "                                    file_path = path_model, \n",
    "                                    bool_instance_eval = True, \n",
    "                                    loss_type = para_loss_type, \n",
    "                                    num_src = len(src_ts_x), \n",
    "                                    snapshot_features = [], \n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [error_tuple], \n",
    "                         ensemble_str = \"Bayesian-one-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_bayes_one\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- bayesian snapshots multi retrain\n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = [tmp_steps[1] for tmp_steps in sort_retrain_hpara_steps],\n",
    "                                    retrain_ids = [i[-2] for i in sort_retrain_hpara_steps[:para_hpara_ensemble_num]],\n",
    "                                    xts = src_ts_x,\n",
    "                                    yts = ts_y,\n",
    "                                    file_path = path_model,\n",
    "                                    bool_instance_eval = True,\n",
    "                                    loss_type = para_loss_type,\n",
    "                                    num_src = len(src_ts_x),\n",
    "                                    snapshot_features = [],\n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error,\n",
    "                         error_tuple = [error_tuple],\n",
    "                         ensemble_str = \"Bayesian-multi-retrain\")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_bayes_multi\" + \".p\", \"wb\"))\n",
    "    \n",
    "    # -- global top1 and topK steps\n",
    "    \n",
    "    retrain_ids, retrain_id_steps = global_top_steps_multi_retrain(retrain_step_error = retrain_hpara_step_error, \n",
    "                                                                   num_step = int(para_test_snapshot_num*para_hpara_ensemble_num))    \n",
    "#     log_test_performance(path = path_log_error, \n",
    "#                          error_tuple = [retrain_ids, retrain_id_steps], \n",
    "#                          ensemble_str = \"Global-top-steps: \")\n",
    "    \n",
    "    error_tuple, py_tuple = testing(retrain_snapshots = retrain_id_steps, \n",
    "                                    retrain_ids = retrain_ids,\n",
    "                                    xts = src_ts_x,\n",
    "                                    yts = ts_y, \n",
    "                                    file_path = path_model,\n",
    "                                    bool_instance_eval = True,\n",
    "                                    loss_type = para_loss_type,\n",
    "                                    num_src = len(src_ts_x), \n",
    "                                    snapshot_features = [], \n",
    "                                    hpara_dict = best_hpara, \n",
    "                                    para_model_type = para_model_type, \n",
    "                                    para_loss_type = para_loss_type)\n",
    "    log_test_performance(path = path_log_error, \n",
    "                         error_tuple = [error_tuple], \n",
    "                         ensemble_str = \"Global-top-steps-multi-retrain \")\n",
    "    # dump predictions\n",
    "    pickle.dump(py_tuple, open(path_py + \"_global\" + \".p\", \"wb\"))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
